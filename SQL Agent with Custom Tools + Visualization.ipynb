{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97cbfc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libs\n",
    "import os, re, glob, json, sqlite3, textwrap, time\n",
    "from IPython.display import Markdown, display\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Data\n",
    "import pandas as pd\n",
    "\n",
    "# LangChain core + tools\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain.tools import tool\n",
    "\n",
    "# Optional schema helpers (list/info) if you want the agent to peek at tables/columns\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_community.tools.sql_database.tool import ListSQLDatabaseTool, InfoSQLDatabaseTool\n",
    "\n",
    "# -- NEW: Visualization & Regression deps --------------------------------------\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib in headless mode for PNG export\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional high-level plotting\n",
    "import seaborn as sns\n",
    "\n",
    "# Stats & time series\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Metrics (optional; will degrade gracefully if absent)\n",
    "try:\n",
    "    from sklearn.metrics import (\n",
    "        r2_score, mean_squared_error, accuracy_score, roc_auc_score,\n",
    "        confusion_matrix, classification_report\n",
    "    )\n",
    "    _SKLEARN_OK = True\n",
    "except Exception:\n",
    "    _SKLEARN_OK = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a73ce39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Groq LLM service…\n",
      "Connected to Groq model 'openai/gpt-oss-20b'.\n"
     ]
    }
   ],
   "source": [
    "from llm_connector import langchain_groq_llm_connector\n",
    "\n",
    "llm = langchain_groq_llm_connector(\"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\",\"openai/gpt-oss-20b\")\n",
    "llm = llm.bind(\n",
    "    tools=[{\"type\":\"browser_search\"},{\"type\":\"code_interpreter\"}],\n",
    "    tool_choice=\"auto\",\n",
    "    reasoning_effort=\"medium\",\n",
    "    top_p=1,\n",
    "    max_completion_tokens=8192,\n",
    ")\n",
    "\n",
    "\n",
    "# ---- Safety knobs for SQL and results ----------------------------------------\n",
    "SQL_TIMEOUT_SECONDS = 15      # wall clock per query\n",
    "SQL_MAX_RETURN_ROWS = 10000   # cap result rows to avoid huge pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c46091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_loader import prepare_citibike_database\n",
    "\n",
    "#db_path, conn, run_query = prepare_citibike_database()\n",
    "DB_PATH = \"/Users/alikarami/Documents/AI Workshop/Sample Data/database.sqlite\"\n",
    "SQLITE_URI = f\"sqlite:///{DB_PATH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8cfdfc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"outputs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3c1e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 2) HELPERS ----------------------------------------------------------------\n",
    "def _is_read_only(sql: str) -> bool:\n",
    "    \"\"\"Allow only SELECT and PRAGMA table_info (read-only).\"\"\"\n",
    "    forbidden = r\"\\b(INSERT|UPDATE|DELETE|DROP|ALTER|CREATE|REPLACE|TRUNCATE|ATTACH|DETACH|VACUUM|BEGIN|COMMIT)\\b\"\n",
    "    if re.search(forbidden, sql, flags=re.IGNORECASE):\n",
    "        return False\n",
    "    if re.search(r\"\\bPRAGMA\\b\", sql, re.IGNORECASE) and \"table_info\" not in sql.lower():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def _add_limit_if_missing(sql: str, limit: int) -> str:\n",
    "    \"\"\"Inject a LIMIT if the user didn't specify one on SELECT queries.\"\"\"\n",
    "    q = sql.strip().rstrip(\";\")\n",
    "    if not q.lower().startswith(\"select\"):\n",
    "        return q + \";\"\n",
    "    if re.search(r\"\\blimit\\b\", q, flags=re.IGNORECASE):\n",
    "        return q + \";\"\n",
    "    return f\"SELECT * FROM (\\n{q}\\n) LIMIT {limit};\"\n",
    "\n",
    "# -- 2a) SQL tool: run a safe read-only query and save to CSV/JSON --------------\n",
    "@tool(\"run_sqlite_query\", return_direct=False)\n",
    "def run_sqlite_query(query: str, limit: int = SQL_MAX_RETURN_ROWS) -> str:\n",
    "    \"\"\"\n",
    "    Run a **read-only** SQL query against the local SQLite DB.\n",
    "    Safety:\n",
    "      - Only SELECT or PRAGMA table_info allowed.\n",
    "      - LIMIT injected if missing (for SELECT).\n",
    "    Returns a small JSON string holding paths & preview.\n",
    "    \"\"\"\n",
    "    if not _is_read_only(query):\n",
    "        return json.dumps({\"error\": \"Only read-only queries allowed (SELECT/PRAGMA table_info).\"})\n",
    "\n",
    "    # Ensure limit for safety\n",
    "    if query.strip().lower().startswith(\"select\") and \"limit\" not in query.lower():\n",
    "        query = _add_limit_if_missing(query, limit)\n",
    "\n",
    "    # Execute with sqlite3; pandas makes it easy to load the result\n",
    "    conn = sqlite3.connect(DB_PATH, check_same_thread=False)\n",
    "    conn.execute(f\"PRAGMA busy_timeout = {SQL_TIMEOUT_SECONDS*1000};\")\n",
    "    try:\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"SQLite error: {e}\"})\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "    # Persist to disk for the next tool\n",
    "    csv_path  = os.path.join(\"outputs\", \"sql_result.csv\")\n",
    "    json_path = os.path.join(\"outputs\", \"sql_result.json\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    df.to_json(json_path, orient=\"records\")\n",
    "\n",
    "    preview_md = df.head(10).to_markdown(index=False) if not df.empty else \"(no rows)\"\n",
    "    return json.dumps({\n",
    "        \"rows\": len(df),\n",
    "        \"columns\": list(df.columns),\n",
    "        \"csv_path\": csv_path,\n",
    "        \"json_path\": json_path,\n",
    "        \"preview\": preview_md,\n",
    "        \"sql_used\": query\n",
    "    })\n",
    "\n",
    "# -- 2b) Pandas tool: simple groupby/aggregate pipeline -------------------------\n",
    "@tool(\"pandas_aggregate\", return_direct=False)\n",
    "def pandas_aggregate(csv_path: str,\n",
    "                     analysis_type: str = \"groupby_aggregate\",\n",
    "                     groupby: Optional[List[str]] = None,\n",
    "                     metrics: Optional[List[Dict[str, str]]] = None,\n",
    "                     filters: Optional[List[Dict[str, Any]]] = None) -> str:\n",
    "    \"\"\"\n",
    "    Load CSV produced by the SQL tool and compute a final table.\n",
    "    Supports:\n",
    "      - analysis_type=\"groupby_aggregate\" (default) with:\n",
    "        * groupby: list of columns\n",
    "        * metrics: list of {\"column\":..., \"agg\": one of sum|mean|count|max|min|median|std|nunique}\n",
    "        * filters: optional list of {\"column\":..,\"op\":..,\"value\":..}\n",
    "      - analysis_type=\"describe\" as a fallback summary\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        return json.dumps({\"error\": f\"CSV not found: {csv_path}\"})\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Quick filter helper\n",
    "    def _apply_filters(df, filters):\n",
    "        if not filters: return df\n",
    "        out = df.copy()\n",
    "        for f in filters:\n",
    "            col, op, val = f.get(\"column\"), f.get(\"op\"), f.get(\"value\")\n",
    "            if col not in out.columns:  # skip unknown columns\n",
    "                continue\n",
    "            if op in (\"==\",\"=\",\"eq\"):\n",
    "                out = out[out[col] == val]\n",
    "            elif op in (\">\",\">=\",\"<\",\"<=\"):\n",
    "                try:\n",
    "                    out = out.query(f\"{col} {op} @val\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "            elif op == \"in\":\n",
    "                val = val if isinstance(val, list) else [val]\n",
    "                out = out[out[col].isin(val)]\n",
    "            elif op == \"contains\":\n",
    "                out = out[out[col].astype(str).str.contains(str(val), na=False)]\n",
    "        return out\n",
    "\n",
    "    if analysis_type == \"describe\":\n",
    "        final_df = df.describe(include=\"all\").reset_index()\n",
    "    else:\n",
    "        # Default: groupby_aggregate\n",
    "        df2 = _apply_filters(df, filters)\n",
    "        if not groupby or not metrics:\n",
    "            final_df = df2.head(50)  # if underspecified, just show sample\n",
    "        else:\n",
    "            agg_map = {}\n",
    "            for m in metrics:\n",
    "                col = m.get(\"column\")\n",
    "                agg = m.get(\"agg\",\"sum\").lower()\n",
    "                if col in df2.columns:\n",
    "                    agg_map.setdefault(col, []).append(agg)\n",
    "            if not agg_map:\n",
    "                final_df = df2.head(50)\n",
    "            else:\n",
    "                out = df2.groupby(groupby).agg(agg_map)\n",
    "                out.columns = [\"__\".join(x if isinstance(x, tuple) else (x,)) if isinstance(x, tuple) else x\n",
    "                               for x in out.columns.values]\n",
    "                final_df = out.reset_index()\n",
    "\n",
    "    result_csv  = os.path.join(\"outputs\", \"final_result.csv\")\n",
    "    result_json = os.path.join(\"outputs\", \"final_result.json\")\n",
    "    final_df.to_csv(result_csv, index=False)\n",
    "    final_df.to_json(result_json, orient=\"records\")\n",
    "\n",
    "    return json.dumps({\n",
    "        \"final_csv\": result_csv,\n",
    "        \"final_json\": result_json,\n",
    "        \"rows\": len(final_df),\n",
    "        \"columns\": list(final_df.columns),\n",
    "        \"preview\": final_df.head(15).to_markdown(index=False)\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44115390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- NEW TOOL: Flexible visualization generator --------------------------------\n",
    "\n",
    "@tool\n",
    "def generate_visualization(spec_json: str) -> str:\n",
    "    \"\"\"\n",
    "    Create one or more charts from a flexible JSON spec and save PNG(s).\n",
    "\n",
    "    Input spec (JSON string). Supported keys:\n",
    "      data_path: str (CSV or JSON file path with records)\n",
    "      data: list[dict] (inline records) -- used if data_path not provided\n",
    "      chart: str  # one of: line|bar|barh|scatter|hist|kde|box|violin|heatmap|area|pie|map_heatmap|map_hexbin\n",
    "      lat_col: str  # required for map_* charts\n",
    "      lng_col: str  # required for map_* charts\n",
    "      weight_col: str  # optional intensity column for map_* charts\n",
    "      tiles: str  # optional folium tileset for map_* charts\n",
    "      zoom_start: int\n",
    "      center: list[float]  # [lat, lng] override for map center\n",
    "      heat_radius: int  # map_heatmap tuning\n",
    "      heat_blur: int\n",
    "      heat_max_zoom: int\n",
    "      min_opacity: float\n",
    "      hex_resolution: int  # map_hexbin tuning\n",
    "      hex_cmap: str\n",
    "      hex_fill_opacity: float\n",
    "      hex_border_weight: float\n",
    "      hex_border_color: str\n",
    "      hex_preview_limit: int\n",
    "      show_points: bool\n",
    "      show_points_limit: int\n",
    "      point_color: str\n",
    "      add_layer_control: bool\n",
    "      x: str      # x-axis column (if applicable)\n",
    "      y: Union[str, list]  # y-axis column(s)\n",
    "      hue: str    # optional grouping column\n",
    "      title: str\n",
    "      bins: int   # for hist\n",
    "      groupby: list[str]  # optional grouping before plotting\n",
    "      agg: Union[str, dict]  # e.g. \"sum\" or {\"revenue\": \"sum\", \"cnt\":\"mean\"}\n",
    "      figsize: list[float]   # e.g. [10,6]\n",
    "      dpi: int\n",
    "      pivot_index: str   # for heatmap (optional)\n",
    "      pivot_columns: str # for heatmap (optional)\n",
    "      pivot_values: str  # for heatmap (optional)\n",
    "\n",
    "    Returns:\n",
    "      JSON string with:\n",
    "        {\n",
    "          \"images\": [\"outputs/visualizations/vis_....png\", ...],  # HTML when map_* charts\n",
    "          \"notes\": \"...\",\n",
    "          \"code\": \"Python code used to render\",\n",
    "          \"rows\": <int>,\n",
    "          \"columns\": [..],\n",
    "          \"preview\": \"markdown table preview (up to 15 rows)\",\n",
    "          \"map_path\": \"outputs/visualizations/map_....html\",  # map_* charts only\n",
    "          \"center\": [lat, lng],  # map_* charts only\n",
    "          \"hex_summary\": [{\"hex_id\": ..., \"value\": ...}]  # map_hexbin only\n",
    "        }\n",
    "    \"\"\"\n",
    "    os.makedirs(\"outputs/visualizations\", exist_ok=True)\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    try:\n",
    "        spec = json.loads(spec_json)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"Invalid JSON spec: {e}\"})\n",
    "\n",
    "    # Load data\n",
    "    df = None\n",
    "    if spec.get(\"data_path\"):\n",
    "        path = spec[\"data_path\"]\n",
    "        try:\n",
    "            if path.lower().endswith(\".csv\"):\n",
    "                df = pd.read_csv(path)\n",
    "            else:\n",
    "                df = pd.read_json(path, orient=\"records\")\n",
    "        except Exception as e:\n",
    "            return json.dumps({\"error\": f\"Failed to load data from {path}: {e}\"})\n",
    "    elif spec.get(\"data\"):\n",
    "        df = pd.DataFrame(spec[\"data\"])\n",
    "    else:\n",
    "        return json.dumps({\"error\": \"Provide either data_path or data records.\"})\n",
    "\n",
    "    # Optional groupby/agg\n",
    "    if spec.get(\"groupby\"):\n",
    "        agg = spec.get(\"agg\", \"sum\")\n",
    "        try:\n",
    "            grouped = df.groupby(spec[\"groupby\"]).agg(agg)\n",
    "            df_plot = grouped.reset_index()\n",
    "        except Exception as e:\n",
    "            return json.dumps({\"error\": f\"Groupby/Aggregation failed: {e}\"})\n",
    "    else:\n",
    "        df_plot = df.copy()\n",
    "\n",
    "    chart = (spec.get(\"chart\") or \"line\").lower()\n",
    "    x = spec.get(\"x\")\n",
    "    y = spec.get(\"y\")\n",
    "    hue = spec.get(\"hue\")\n",
    "    title = spec.get(\"title\", f\"{chart.title()} chart\")\n",
    "    bins = spec.get(\"bins\", 20)\n",
    "    figsize = tuple(spec.get(\"figsize\", [10, 6]))\n",
    "    dpi = int(spec.get(\"dpi\", 150))\n",
    "\n",
    "    images = []\n",
    "    code_snippets = []\n",
    "\n",
    "\n",
    "    map_chart_types = {\"map_heatmap\", \"geo_heatmap\", \"map_hexbin\", \"geo_hexbin\", \"map_hex\", \"geo_hex\"}\n",
    "    if chart in map_chart_types:\n",
    "        try:\n",
    "            import folium\n",
    "            from folium.plugins import HeatMap\n",
    "        except ImportError:\n",
    "            return json.dumps({\"error\": \"Missing dependency `folium`. Install it with `pip install folium`.\"})\n",
    "\n",
    "        lat_col = spec.get(\"lat_col\") or spec.get(\"latitude\") or spec.get(\"lat\")\n",
    "        lng_col = spec.get(\"lng_col\") or spec.get(\"longitude\") or spec.get(\"lon\") or spec.get(\"lng\")\n",
    "        if not lat_col or not lng_col:\n",
    "            return json.dumps({\"error\": \"Geospatial charts require `lat_col` and `lng_col`.\"})\n",
    "\n",
    "        missing = [col for col in (lat_col, lng_col) if col not in df_plot.columns]\n",
    "        if missing:\n",
    "            return json.dumps({\"error\": f\"Missing latitude/longitude columns: {missing}\"})\n",
    "\n",
    "        weight_col = spec.get(\"weight_col\")\n",
    "        cols_to_numeric = [lat_col, lng_col]\n",
    "        if weight_col:\n",
    "            cols_to_numeric.append(weight_col)\n",
    "\n",
    "        df_geo = df_plot.copy()\n",
    "        for col in cols_to_numeric:\n",
    "            df_geo[col] = pd.to_numeric(df_geo[col], errors=\"coerce\")\n",
    "\n",
    "        df_geo = df_geo.dropna(subset=[lat_col, lng_col])\n",
    "        if weight_col:\n",
    "            df_geo = df_geo.dropna(subset=[weight_col])\n",
    "        if df_geo.empty:\n",
    "            return json.dumps({\"error\": \"No valid latitude/longitude rows after cleaning.\"})\n",
    "\n",
    "        center = spec.get(\"center\")\n",
    "        if center and len(center) == 2:\n",
    "            center_lat, center_lng = float(center[0]), float(center[1])\n",
    "        else:\n",
    "            center_lat = float(df_geo[lat_col].mean())\n",
    "            center_lng = float(df_geo[lng_col].mean())\n",
    "\n",
    "        tiles = spec.get(\"tiles\", \"cartodbpositron\")\n",
    "        zoom_start = int(spec.get(\"zoom_start\", 12))\n",
    "        fmap = folium.Map(location=[center_lat, center_lng], tiles=tiles, zoom_start=zoom_start)\n",
    "\n",
    "        map_path = os.path.join(\"outputs/visualizations\", f\"map_{chart}_{ts}.html\")\n",
    "        hex_summary = None\n",
    "\n",
    "        if \"hex\" in chart:\n",
    "            try:\n",
    "                import h3\n",
    "            except ImportError:\n",
    "                return json.dumps({\"error\": \"Missing dependency `h3`. Install it with `pip install h3`.\"})\n",
    "\n",
    "            res = int(spec.get(\"hex_resolution\", 8))\n",
    "            df_geo[\"__hex__\"] = df_geo.apply(lambda row: h3.geo_to_h3(row[lat_col], row[lng_col], res), axis=1)\n",
    "\n",
    "            if weight_col:\n",
    "                aggregated = df_geo.groupby(\"__hex__\")[weight_col].sum()\n",
    "            else:\n",
    "                aggregated = df_geo.groupby(\"__hex__\").size()\n",
    "            aggregated = aggregated[aggregated > 0]\n",
    "            if aggregated.empty:\n",
    "                return json.dumps({\"error\": \"Hexbin aggregation produced no tiles.\"})\n",
    "\n",
    "            from matplotlib import cm, colors\n",
    "            cmap = cm.get_cmap(spec.get(\"hex_cmap\", \"YlOrRd\"))\n",
    "            norm = colors.Normalize(vmin=float(aggregated.min()), vmax=float(aggregated.max()))\n",
    "\n",
    "            hex_summary = []\n",
    "            for hex_id, value in aggregated.items():\n",
    "                boundary = h3.h3_to_geo_boundary(hex_id, geo_json=True)\n",
    "                color = colors.to_hex(cmap(norm(float(value))))\n",
    "                folium.Polygon(\n",
    "                    locations=boundary,\n",
    "                    fill=True,\n",
    "                    fill_color=color,\n",
    "                    fill_opacity=float(spec.get(\"hex_fill_opacity\", 0.6)),\n",
    "                    weight=float(spec.get(\"hex_border_weight\", 0.4)),\n",
    "                    color=spec.get(\"hex_border_color\", \"#555555\"),\n",
    "                    tooltip=f\"{hex_id} -> {float(value):.2f}\",\n",
    "                ).add_to(fmap)\n",
    "                hex_summary.append({\"hex_id\": hex_id, \"value\": float(value)})\n",
    "\n",
    "            notes = f\"Rendered {len(aggregated)} H3 hexagons at resolution {res}.\"\n",
    "        else:\n",
    "            if weight_col:\n",
    "                heat_data = df_geo[[lat_col, lng_col, weight_col]].values.tolist()\n",
    "            else:\n",
    "                heat_data = df_geo[[lat_col, lng_col]].values.tolist()\n",
    "\n",
    "            HeatMap(\n",
    "                heat_data,\n",
    "                name=spec.get(\"layer_name\", \"intensity\"),\n",
    "                radius=int(spec.get(\"heat_radius\", 14)),\n",
    "                blur=int(spec.get(\"heat_blur\", 20)),\n",
    "                max_zoom=int(spec.get(\"heat_max_zoom\", 16)),\n",
    "                min_opacity=float(spec.get(\"min_opacity\", 0.2)),\n",
    "            ).add_to(fmap)\n",
    "\n",
    "            notes = f\"Rendered Leaflet heatmap with {len(df_geo)} points.\"\n",
    "\n",
    "        if spec.get(\"show_points\"):\n",
    "            limit = int(spec.get(\"show_points_limit\", 2500))\n",
    "            color = spec.get(\"point_color\", \"#1f77b4\")\n",
    "            for _, row in df_geo.head(limit).iterrows():\n",
    "                folium.CircleMarker(\n",
    "                    location=[row[lat_col], row[lng_col]],\n",
    "                    radius=2,\n",
    "                    color=color,\n",
    "                    fill=True,\n",
    "                    fill_opacity=0.7,\n",
    "                ).add_to(fmap)\n",
    "\n",
    "        if spec.get(\"add_layer_control\", True):\n",
    "            folium.LayerControl().add_to(fmap)\n",
    "\n",
    "        fmap.save(map_path)\n",
    "\n",
    "        preview_cols = [lat_col, lng_col] + ([weight_col] if weight_col else [])\n",
    "        preview_df = df_geo[preview_cols].head(15)\n",
    "\n",
    "        snippet = textwrap.dedent(f\"\"\"\n",
    "        # chart={chart}\n",
    "        # tiles={tiles}\n",
    "        # saved -> {map_path}\n",
    "        \"\"\").strip()\n",
    "        code_snippets.append(snippet)\n",
    "\n",
    "        response = {\n",
    "            \"images\": [map_path],\n",
    "            \"notes\": notes + \" Saved interactive map HTML.\",\n",
    "            \"code\": \"\\n\\n\".join(code_snippets),\n",
    "            \"rows\": int(len(df_geo)),\n",
    "            \"columns\": list(df_geo.columns),\n",
    "            \"preview\": preview_df.to_markdown(index=False),\n",
    "            \"center\": [center_lat, center_lng],\n",
    "            \"map_path\": map_path,\n",
    "        }\n",
    "        if hex_summary:\n",
    "            limit = int(spec.get(\"hex_preview_limit\", 25))\n",
    "            response[\"hex_summary\"] = hex_summary[:limit]\n",
    "\n",
    "        return json.dumps(response)\n",
    "\n",
    "    # fall through to standard matplotlib handling if chart not map_*\n",
    "    def _save(fig, name):\n",
    "        out = os.path.join(\"outputs/visualizations\", name)\n",
    "        fig.savefig(out, dpi=dpi, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        images.append(out)\n",
    "\n",
    "    # One chart per y if a list was provided\n",
    "    y_cols = y if isinstance(y, list) else ([y] if y is not None else [None])\n",
    "\n",
    "    for idx, ycol in enumerate(y_cols):\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "        # Choose plotting backend\n",
    "        if chart in (\"line\", \"area\"):\n",
    "            if hue:\n",
    "                sns.lineplot(data=df_plot, x=x, y=ycol, hue=hue, ax=ax)\n",
    "            else:\n",
    "                sns.lineplot(data=df_plot, x=x, y=ycol, ax=ax)\n",
    "            if chart == \"area\":\n",
    "                ax.fill_between(df_plot[x], df_plot[ycol], alpha=0.3)\n",
    "\n",
    "        elif chart in (\"bar\", \"barh\"):\n",
    "            if hue:\n",
    "                sns.barplot(data=df_plot, x=x, y=ycol, hue=hue, ax=ax)\n",
    "            else:\n",
    "                sns.barplot(data=df_plot, x=x, y=ycol, ax=ax)\n",
    "            if chart == \"barh\":\n",
    "                ax.invert_xaxis()  # indicate horizontal orientation\n",
    "\n",
    "        elif chart == \"scatter\":\n",
    "            if hue:\n",
    "                sns.scatterplot(data=df_plot, x=x, y=ycol, hue=hue, ax=ax)\n",
    "            else:\n",
    "                sns.scatterplot(data=df_plot, x=x, y=ycol, ax=ax)\n",
    "\n",
    "        elif chart == \"hist\":\n",
    "            sns.histplot(data=df_plot, x=x or ycol, bins=bins, ax=ax)\n",
    "\n",
    "        elif chart == \"kde\":\n",
    "            sns.kdeplot(data=df_plot, x=x or ycol, ax=ax, fill=True)\n",
    "\n",
    "        elif chart == \"box\":\n",
    "            sns.boxplot(data=df_plot, x=x, y=ycol, hue=hue, ax=ax)\n",
    "\n",
    "        elif chart == \"violin\":\n",
    "            sns.violinplot(data=df_plot, x=x, y=ycol, hue=hue, split=False, ax=ax)\n",
    "\n",
    "        elif chart == \"pie\":\n",
    "            if ycol is None:\n",
    "                return json.dumps({\"error\": \"pie requires 'y' values.\"})\n",
    "            data = df_plot[ycol]\n",
    "            labels = df_plot[x] if x else None\n",
    "            ax.pie(data, labels=labels, autopct=\"%1.1f%%\")\n",
    "            ax.axis(\"equal\")\n",
    "\n",
    "        elif chart == \"heatmap\":\n",
    "            p_idx = spec.get(\"pivot_index\")\n",
    "            p_cols = spec.get(\"pivot_columns\")\n",
    "            p_vals = spec.get(\"pivot_values\")\n",
    "            if not (p_idx and p_cols and p_vals):\n",
    "                return json.dumps({\"error\": \"heatmap requires pivot_index, pivot_columns, pivot_values\"})\n",
    "            mat = df_plot.pivot(index=p_idx, columns=p_cols, values=p_vals)\n",
    "            sns.heatmap(mat, ax=ax)\n",
    "\n",
    "        else:\n",
    "            plt.close(fig)\n",
    "            return json.dumps({\"error\": f\"Unsupported chart type: {chart}\"})\n",
    "\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(x if x else \"\")\n",
    "        ax.set_ylabel(ycol if ycol else \"\")\n",
    "\n",
    "        fname = f\"vis_{chart}_{idx}_{ts}.png\"\n",
    "        _save(fig, fname)\n",
    "\n",
    "        # Log a minimal \"code used\" for auditability\n",
    "        code_snippets.append(\n",
    "            f\"# chart={chart}\\n# title={title}\\n# hue={hue}\\n# x={x}\\n# y={ycol}\\n\"\n",
    "            f\"# saved -> outputs/visualizations/{fname}\"\n",
    "        )\n",
    "\n",
    "    preview = df_plot.head(15).to_markdown(index=False)\n",
    "    return json.dumps({\n",
    "        \"images\": images,\n",
    "        \"notes\": f\"Generated {len(images)} {chart} chart(s).\",\n",
    "        \"code\": \"\\n\\n\".join(code_snippets),\n",
    "        \"rows\": int(len(df_plot)),\n",
    "        \"columns\": list(df_plot.columns),\n",
    "        \"preview\": preview\n",
    "    })\n",
    "\n",
    "\n",
    "# -- NEW TOOL: Regression / correlation / time series analysis ------------------\n",
    "@tool\n",
    "def regression_analysis(spec_json: str) -> str:\n",
    "    \"\"\"\n",
    "    Perform correlation, linear/logistic regression, or ARIMA/SARIMAX.\n",
    "\n",
    "    Input spec (JSON string). Keys:\n",
    "      data_path: str (CSV/JSON path) or data: list[dict]\n",
    "      type: \"correlation\" | \"linear\" | \"logistic\" | \"arima\" | \"sarimax\"\n",
    "      # For linear/logistic:\n",
    "      formula: str  # e.g. \"y ~ x1 + x2\" (preferred)\n",
    "      y: str        # if no formula\n",
    "      X: list[str]  # if no formula\n",
    "      add_constant: bool (default True)\n",
    "      # For time series:\n",
    "      y: str             # dependent series\n",
    "      order: [p, d, q]   # ARIMA/SARIMAX\n",
    "      seasonal_order: [P, D, Q, s]  # for SARIMAX only\n",
    "      exog: list[str]    # optional exogenous regressors\n",
    "      # General:\n",
    "      dropna: bool (default True)\n",
    "      test_size: float (0..1, optional, for basic split)\n",
    "      standardize: bool (False)  # only for linear/logistic\n",
    "\n",
    "    Returns JSON with paths to artifacts, metrics, and a short summary.\n",
    "    \"\"\"\n",
    "    os.makedirs(\"outputs/regression\", exist_ok=True)\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    try:\n",
    "        spec = json.loads(spec_json)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"Invalid JSON: {e}\"})\n",
    "\n",
    "    # Load data\n",
    "    if spec.get(\"data_path\"):\n",
    "        path = spec[\"data_path\"]\n",
    "        try:\n",
    "            if path.lower().endswith(\".csv\"):\n",
    "                df = pd.read_csv(path)\n",
    "            else:\n",
    "                df = pd.read_json(path, orient=\"records\")\n",
    "        except Exception as e:\n",
    "            return json.dumps({\"error\": f\"Failed to load data from {path}: {e}\"})\n",
    "    elif spec.get(\"data\"):\n",
    "        df = pd.DataFrame(spec[\"data\"])\n",
    "    else:\n",
    "        return json.dumps({\"error\": \"Provide data_path or inline data.\"})\n",
    "\n",
    "    if spec.get(\"dropna\", True):\n",
    "        df = df.dropna()\n",
    "\n",
    "    a_type = (spec.get(\"type\") or \"linear\").lower()\n",
    "    artifacts = []\n",
    "    out_dir = os.path.join(\"outputs\", \"regression\", f\"run_{a_type}_{ts}\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    def _savefig(fig, name):\n",
    "        p = os.path.join(out_dir, name)\n",
    "        fig.savefig(p, dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        artifacts.append(p)\n",
    "\n",
    "    report = {\"type\": a_type, \"artifacts\": artifacts, \"code\": \"\", \"metrics\": {}, \"model_summary_path\": None}\n",
    "\n",
    "    # ---- correlation ----------------------------------------------------------\n",
    "    if a_type == \"correlation\":\n",
    "        corr = df.corr(numeric_only=True)\n",
    "        corr_csv = os.path.join(out_dir, \"correlation_matrix.csv\")\n",
    "        corr.to_csv(corr_csv)\n",
    "        report[\"artifacts\"].append(corr_csv)\n",
    "\n",
    "        # Heatmap\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        sns.heatmap(corr, annot=False, ax=ax)\n",
    "        ax.set_title(\"Correlation heatmap\")\n",
    "        _savefig(fig, \"corr_heatmap.png\")\n",
    "\n",
    "        report[\"summary\"] = \"Computed Pearson correlation matrix.\"\n",
    "        report[\"code\"] = \"corr = df.corr(numeric_only=True)\"\n",
    "        return json.dumps(report)\n",
    "\n",
    "    # ---- regression helpers ---------------------------------------------------\n",
    "    def _standardize(frame, cols):\n",
    "        mu = frame[cols].mean()\n",
    "        sd = frame[cols].std().replace(0, 1.0)\n",
    "        frame[cols] = (frame[cols] - mu) / sd\n",
    "        return frame\n",
    "\n",
    "    # ---- linear / logistic ----------------------------------------------------\n",
    "    if a_type in (\"linear\", \"logistic\"):\n",
    "        formula = spec.get(\"formula\")\n",
    "        add_const = spec.get(\"add_constant\", True)\n",
    "        standardize = bool(spec.get(\"standardize\", False))\n",
    "\n",
    "        if formula:\n",
    "            y_name = formula.split(\"~\")[0].strip()\n",
    "            X_cols = None\n",
    "        else:\n",
    "            y_name = spec.get(\"y\")\n",
    "            X_cols = spec.get(\"X\") or []\n",
    "            if not (y_name and X_cols):\n",
    "                return json.dumps({\"error\": \"Provide formula or y + X for regression.\"})\n",
    "\n",
    "        data = df.copy()\n",
    "        if not formula and standardize and a_type == \"linear\":\n",
    "            data = _standardize(data, X_cols)\n",
    "\n",
    "        # Fit\n",
    "        if a_type == \"linear\":\n",
    "            if formula:\n",
    "                model = smf.ols(formula=formula, data=data).fit()\n",
    "                report[\"code\"] = f\"smf.ols(formula='{formula}', data=df).fit()\"\n",
    "            else:\n",
    "                X = data[X_cols].copy()\n",
    "                y = data[y_name].copy()\n",
    "                if add_const:\n",
    "                    X = sm.add_constant(X, has_constant='add')\n",
    "                model = sm.OLS(y, X).fit()\n",
    "                report[\"code\"] = f\"sm.OLS(y, sm.add_constant(X)).fit()\"\n",
    "            summary_txt = model.summary().as_text()\n",
    "            rep_path = os.path.join(out_dir, \"ols_summary.txt\")\n",
    "            with open(rep_path, \"w\") as f:\n",
    "                f.write(summary_txt)\n",
    "            report[\"model_summary_path\"] = rep_path\n",
    "\n",
    "            # Basic diagnostics\n",
    "            try:\n",
    "                fitted = model.fittedvalues\n",
    "                resid = model.resid\n",
    "                fig, ax = plt.subplots(figsize=(8,5))\n",
    "                ax.scatter(fitted, resid)\n",
    "                ax.axhline(0, ls=\"--\")\n",
    "                ax.set_xlabel(\"Fitted\")\n",
    "                ax.set_ylabel(\"Residuals\")\n",
    "                ax.set_title(\"Residuals vs Fitted\")\n",
    "                _savefig(fig, \"resid_vs_fitted.png\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # Metrics & coefficients\n",
    "            try:\n",
    "                report[\"metrics\"] = {\n",
    "                    \"r2\": float(model.rsquared),\n",
    "                    \"adj_r2\": float(model.rsquared_adj),\n",
    "                    \"aic\": float(model.aic),\n",
    "                    \"bic\": float(model.bic)\n",
    "                }\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                params = model.params.to_dict()\n",
    "                report[\"coefficients\"] = {k: float(v) for k, v in params.items()}\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            report[\"summary\"] = \"Linear regression (OLS) completed.\"\n",
    "\n",
    "        else:  # logistic\n",
    "            if formula:\n",
    "                model = smf.logit(formula=formula, data=data).fit(disp=False)\n",
    "                report[\"code\"] = f\"smf.logit(formula='{formula}', data=df).fit()\"\n",
    "                y_name = formula.split(\"~\")[0].strip()\n",
    "            else:\n",
    "                X = data[X_cols].copy()\n",
    "                y = data[y_name].copy()\n",
    "                if add_const:\n",
    "                    X = sm.add_constant(X, has_constant='add')\n",
    "                model = sm.Logit(y, X).fit(disp=False)\n",
    "                report[\"code\"] = f\"sm.Logit(y, sm.add_constant(X)).fit()\"\n",
    "\n",
    "            summary_txt = model.summary().as_text()\n",
    "            rep_path = os.path.join(out_dir, \"logit_summary.txt\")\n",
    "            with open(rep_path, \"w\") as f:\n",
    "                f.write(summary_txt)\n",
    "            report[\"model_summary_path\"] = rep_path\n",
    "\n",
    "            # Predictions and simple metrics if sklearn present\n",
    "            try:\n",
    "                pred_probs = model.predict()\n",
    "                y_true = data[y_name].astype(int)\n",
    "                y_pred = (pred_probs >= 0.5).astype(int)\n",
    "                metrics = {\"threshold\": 0.5}\n",
    "                if _SKLEARN_OK:\n",
    "                    metrics.update({\n",
    "                        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "                        \"roc_auc\": float(roc_auc_score(y_true, pred_probs)),\n",
    "                    })\n",
    "                report[\"metrics\"] = metrics\n",
    "\n",
    "                # Confusion matrix plot\n",
    "                if _SKLEARN_OK:\n",
    "                    cm = confusion_matrix(y_true, y_pred)\n",
    "                    fig, ax = plt.subplots(figsize=(5,4))\n",
    "                    sns.heatmap(cm, annot=True, fmt=\"d\", ax=ax)\n",
    "                    ax.set_xlabel(\"Predicted\")\n",
    "                    ax.set_ylabel(\"Actual\")\n",
    "                    ax.set_title(\"Confusion Matrix\")\n",
    "                    _savefig(fig, \"confusion_matrix.png\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            report[\"summary\"] = \"Logistic regression completed.\"\n",
    "\n",
    "        return json.dumps(report)\n",
    "\n",
    "    # ---- ARIMA / SARIMAX ------------------------------------------------------\n",
    "    if a_type in (\"arima\", \"sarimax\"):\n",
    "        y_name = spec.get(\"y\")\n",
    "        if not y_name:\n",
    "            return json.dumps({\"error\": \"Provide 'y' for time series.\"})\n",
    "        order = spec.get(\"order\")\n",
    "        if not order or len(order) != 3:\n",
    "            return json.dumps({\"error\": \"Provide ARIMA order [p,d,q].\"})\n",
    "        exog_cols = spec.get(\"exog\") or []\n",
    "        seasonal_order = spec.get(\"seasonal_order\")\n",
    "\n",
    "        y = df[y_name].astype(float)\n",
    "        exog = df[exog_cols] if exog_cols else None\n",
    "\n",
    "        if a_type == \"arima\":\n",
    "            model = SARIMAX(endog=y, order=tuple(order), exog=exog, enforce_stationarity=False, enforce_invertibility=False)\n",
    "        else:\n",
    "            if not seasonal_order or len(seasonal_order) != 4:\n",
    "                return json.dumps({\"error\": \"Provide seasonal_order [P,D,Q,s] for SARIMAX.\"})\n",
    "            model = SARIMAX(endog=y, order=tuple(order), seasonal_order=tuple(seasonal_order),\n",
    "                            exog=exog, enforce_stationarity=False, enforce_invertibility=False)\n",
    "        res = model.fit(disp=False)\n",
    "\n",
    "        # Save summary\n",
    "        rep_path = os.path.join(out_dir, \"ts_summary.txt\")\n",
    "        with open(rep_path, \"w\") as f:\n",
    "            f.write(res.summary().as_text())\n",
    "        report[\"model_summary_path\"] = rep_path\n",
    "\n",
    "        # Forecast next steps if requested\n",
    "        steps = int(spec.get(\"forecast_steps\", 0))\n",
    "        if steps > 0:\n",
    "            if exog is not None:\n",
    "                # naive exog continuation: repeat last row\n",
    "                exog_fc = pd.concat([exog] + [exog.iloc[[-1]]] * steps, ignore_index=True).tail(steps)\n",
    "                fc = res.get_forecast(steps=steps, exog=exog_fc)\n",
    "            else:\n",
    "                fc = res.get_forecast(steps=steps)\n",
    "            fc_mean = fc.predicted_mean\n",
    "            fc_ci = fc.conf_int()\n",
    "\n",
    "            # Plot\n",
    "            fig, ax = plt.subplots(figsize=(10,5))\n",
    "            y.plot(ax=ax, label=\"observed\")\n",
    "            fc_mean.plot(ax=ax, label=\"forecast\")\n",
    "            ax.fill_between(fc_ci.index, fc_ci.iloc[:,0], fc_ci.iloc[:,1], alpha=0.2)\n",
    "            ax.set_title(\"Forecast\")\n",
    "            ax.legend()\n",
    "            _savefig(fig, \"forecast.png\")\n",
    "\n",
    "            # Save CSV\n",
    "            fc_out = pd.DataFrame({\"forecast\": fc_mean})\n",
    "            fc_csv = os.path.join(out_dir, \"forecast.csv\")\n",
    "            fc_out.to_csv(fc_csv)\n",
    "            report[\"artifacts\"].append(fc_csv)\n",
    "\n",
    "        # Metrics\n",
    "        report[\"metrics\"] = {\"aic\": float(res.aic), \"bic\": float(res.bic)}\n",
    "        report[\"code\"] = f\"SARIMAX(order={tuple(order)}, seasonal_order={tuple(seasonal_order) if seasonal_order else None})\"\n",
    "        report[\"summary\"] = f\"{a_type.upper()} model fitted.\"\n",
    "\n",
    "        return json.dumps(report)\n",
    "\n",
    "    return json.dumps({\"error\": f\"Unknown analysis type '{a_type}'\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1705e89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent ready.\n"
     ]
    }
   ],
   "source": [
    "# -- 3) PROMPT + AGENT ---------------------------------------------------------\n",
    "# Optional: give the agent quick access to schema overview and table info.\n",
    "db = SQLDatabase.from_uri(SQLITE_URI, sample_rows_in_table_info=2)\n",
    "list_tables_tool = ListSQLDatabaseTool(db=db)\n",
    "info_table_tool  = InfoSQLDatabaseTool(db=db)\n",
    "\n",
    "SYSTEM_RULES = textwrap.dedent(\"\"\"\n",
    "You are a careful data analyst working with a local SQLite database.\n",
    "You MUST follow these rules:\n",
    "\n",
    "- SAFETY: Only run read-only SQL (SELECT or PRAGMA table_info). No writes or DDL.\n",
    "- EFFICIENCY: \n",
    "  - Avoid huge pulls. If the user didn't specify limits/time windows, start small.\n",
    "  - Only query on the SQL DB tables, and when in need to manipulate the output data of a query, use the LLM by providing the data and instructing it to run the python pandas code for analysis and returning the data to you.\n",
    "  - In case the SQL db returns an error, try to fix it by manipulating the query or the data itself, but maintain the analysis integrity.\n",
    "  - In case of failure, in the summary give the explaination of why the analysis failed.\n",
    "- CHOOSE METHOD: Decide the simplest correct analysis (groupby aggregate, describe, or basic stats).\n",
    "- PIPELINE:\n",
    "  1) In case any metrics are requested to be calculated, clarify your formula for that metrics first.\n",
    "  2) In case any visualizations are requested, clarify your chart type and the columns to be used first.\n",
    "  3) For the metrics and visualizations only, try to search online for refernces of those metrics/visualizations and clarify your references.\n",
    "  4) Inspect schema with list_tables/info if unsure. Decide which tables provide the info you need, including what joins are neccessary.\n",
    "  5) Write a SELECT query with the minimal set of columns and filters needed.\n",
    "  6) Call `run_sqlite_query` to execute and save results.\n",
    "  7) If aggregation is needed, call `pandas_aggregate` with a structured spec.\n",
    "  8) Use `generate_visualization` to produce any plots in case the user asks for visualizations. Save PNGs under outputs/visualizations/.\n",
    "  9) Use `regression_analysis` for correlations, linear/logistic regression, and ARIMA/SARIMAX time series.\n",
    "- OUTPUT FORMAT:\n",
    "  - Start with a short PLAN (bullets).\n",
    "  - Show the SQL used (fenced code).\n",
    "  - Show a preview table of the final result.\n",
    "  - End with a crisp, non-verbose paragraph describing the data to the user.\n",
    "  - Always produce a final numeric table as a Pandas DataFrame named `final_df`.\n",
    "  - The pipeline will save `outputs/final_result.csv` and `outputs/final_result.json`.\n",
    "  - If you created charts, their PNG paths must be returned from the visualization tool.\n",
    "\"\"\").strip()\n",
    "\n",
    "# The prompt ensures the agent builds a simple plan + uses tools in-order\n",
    "PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_RULES),\n",
    "    (\"human\", \"User analysis request: {analysis_request}\\n\\nProceed.\"),\n",
    "    MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "# Attach both tools to the same agent\n",
    "tools = [list_tables_tool, info_table_tool, run_sqlite_query, pandas_aggregate, generate_visualization, regression_analysis]\n",
    "\n",
    "# Create a tool-calling agent with our prompt\n",
    "agent_runnable = create_tool_calling_agent(llm, tools, PROMPT)\n",
    "\n",
    "# Wrap it in an executor so we can capture intermediate steps (the “action trace”)\n",
    "executor = AgentExecutor(\n",
    "    agent=agent_runnable,\n",
    "    tools=tools,\n",
    "    verbose=True,                     # prints a readable trace in the cell output\n",
    "    return_intermediate_steps=True,   # lets us programmatically inspect tool calls\n",
    "    max_iterations=8,\n",
    "    handle_parsing_errors=True,\n",
    ")\n",
    "print(\"Agent ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a0e952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `sql_db_list_tables` with `{'tool_input': ''}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mstation, status, trip, weather\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `run_sqlite_query` with `{'query': 'SELECT round(s.latitude,2) as lat_bin, round(s.longitude,2) as lon_bin, COUNT(t.trip_id) as arrivals FROM trip t JOIN station s ON t.end_station_id = s.station_id GROUP BY lat_bin, lon_bin;'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m{\"error\": \"SQLite error: Execution failed on sql 'SELECT * FROM (\\nSELECT round(s.latitude,2) as lat_bin, round(s.longitude,2) as lon_bin, COUNT(t.trip_id) as arrivals FROM trip t JOIN station s ON t.end_station_id = s.station_id GROUP BY lat_bin, lon_bin\\n) LIMIT 10000;': no such column: s.latitude\"}\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `sql_db_schema` with `{'table_names': 'station'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m\n",
      "CREATE TABLE station (\n",
      "\tid INTEGER, \n",
      "\tname TEXT, \n",
      "\tlat NUMERIC, \n",
      "\tlong NUMERIC, \n",
      "\tdock_count INTEGER, \n",
      "\tcity TEXT, \n",
      "\tinstallation_date TEXT, \n",
      "\tPRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "/*\n",
      "2 rows from station table:\n",
      "id\tname\tlat\tlong\tdock_count\tcity\tinstallation_date\n",
      "2\tSan Jose Diridon Caltrain Station\t37.3297320000\t-121.9017820000\t27\tSan Jose\t8/6/2013\n",
      "3\tSan Jose Civic Center\t37.3306980000\t-121.8889790000\t15\tSan Jose\t8/5/2013\n",
      "*/\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `run_sqlite_query` with `{'query': 'SELECT s.lat, s.long, COUNT(t.trip_id) as arrivals FROM trip t JOIN station s ON t.end_station_id = s.id GROUP BY s.id;'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m{\"error\": \"SQLite error: Execution failed on sql 'SELECT * FROM (\\nSELECT s.lat, s.long, COUNT(t.trip_id) as arrivals FROM trip t JOIN station s ON t.end_station_id = s.id GROUP BY s.id\\n) LIMIT 10000;': no such column: t.trip_id\"}\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `sql_db_schema` with `{'table_names': 'trip'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m\n",
      "CREATE TABLE trip (\n",
      "\tid INTEGER, \n",
      "\tduration INTEGER, \n",
      "\tstart_date TEXT, \n",
      "\tstart_station_name TEXT, \n",
      "\tstart_station_id INTEGER, \n",
      "\tend_date TEXT, \n",
      "\tend_station_name TEXT, \n",
      "\tend_station_id INTEGER, \n",
      "\tbike_id INTEGER, \n",
      "\tsubscription_type TEXT, \n",
      "\tzip_code INTEGER, \n",
      "\tPRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "/*\n",
      "2 rows from trip table:\n",
      "id\tduration\tstart_date\tstart_station_name\tstart_station_id\tend_date\tend_station_name\tend_station_id\tbike_id\tsubscription_type\tzip_code\n",
      "4069\t174\t8/29/2013 9:08\t2nd at South Park\t64\t8/29/2013 9:11\t2nd at South Park\t64\t288\tSubscriber\t94114\n",
      "4073\t1067\t8/29/2013 9:24\tSouth Van Ness at Market\t66\t8/29/2013 9:42\tSan Francisco Caltrain 2 (330 Townsend)\t69\t321\tSubscriber\t94703\n",
      "*/\u001b[0m"
     ]
    }
   ],
   "source": [
    "# -- 4) RUN THE AGENT ----------------------------------------------------------\n",
    "# Try your own request here. A few examples:\n",
    "# \"What is the average utilization rate of the bikes for each station in Palo Alto? Meaning that how much each station's bikes are utilized by time?\"\n",
    "# \"What are the top 10 stations by departures on weekdays 7–10 AM?\"\n",
    "# \"Create a heatmap of station arrivals over a map of the city.\" -> \"Create a heatmap of station arrivals over a map of the san fransisco.\"\n",
    "# \"Create a regression analysis between weather features and number of trips of each station. Analyze the statistical significance of the results.\"\n",
    "\n",
    "analysis_request = \"Create a heatmap of station arrivals over a map of the san fransisco.\"\n",
    "\n",
    "result = executor.invoke({\"analysis_request\": analysis_request})\n",
    "\n",
    "# The AgentExecutor returns:\n",
    "# - 'output' (final text answer)\n",
    "# - 'intermediate_steps' ([(AgentAction, observation), ...]) = our action trace\n",
    "print(\"\\n=== FINAL ANSWER ===\")\n",
    "print(result[\"output\"])\n",
    "\n",
    "print(\"\\n=== ACTION TRACE (tools & observations) ===\")\n",
    "for i, step in enumerate(result[\"intermediate_steps\"], 1):\n",
    "    action, observation = step\n",
    "    print(f\"\\n-- Step {i}:\")\n",
    "    print(\"Tool:\", getattr(action, \"tool\", \"<unknown>\"))\n",
    "    print(\"Tool Input:\", getattr(action, \"tool_input\", {}))\n",
    "    # Observations can be large JSON strings; truncate for readability\n",
    "    obs_str = str(observation)\n",
    "    print(\"Observation:\", (obs_str[:800] + \"...\") if len(obs_str) > 800 else obs_str)\n",
    "    print(\"Final Answer:\", result[\"output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defc52dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Clarification Needed**\n",
       "\n",
       "To create a heatmap of station arrivals over a city map, I need a bit more detail:\n",
       "\n",
       "1. **City / Map** – Which city’s map should the heatmap be plotted on? (e.g., New York, Chicago, etc.)  \n",
       "2. **Station Data** – Does the `station` table contain latitude/longitude columns (e.g., `lat`, `lon`, `latitude`, `longitude`)?  \n",
       "3. **Arrival Definition** – Should arrivals be counted from the `trip` table’s `arrival_station_id` (or similar) column?  \n",
       "4. **Time Window** – Do you want arrivals for all time, a specific date range, or a particular month?  \n",
       "5. **Heatmap Granularity** – Do you prefer a simple point‑density heatmap (one point per station weighted by arrivals) or a grid‑based density overlay?\n",
       "\n",
       "Once I have this information, I can:\n",
       "- Pull the necessary station coordinates and arrival counts,\n",
       "- Aggregate the data,\n",
       "- Generate a heatmap overlay on the requested city map."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(result[\"output\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72731f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
