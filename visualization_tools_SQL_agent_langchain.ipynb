{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe75ffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/alikarami76/genai-workshop/refs/heads/main/db_loader.py\n",
    "!wget -q https://raw.githubusercontent.com/alikarami76/genai-workshop/refs/heads/main/llm_connector.py\n",
    "!wget -q https://raw.githubusercontent.com/alikarami76/genai-workshop/refs/heads/main/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518bd163",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cbfc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libs\n",
    "import os, re, glob, json, sqlite3, textwrap,time\n",
    "from IPython.display import Markdown, display\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Data\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization libs\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # headless backend for PNG\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# LangChain core + tools\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain.tools import tool\n",
    "\n",
    "# Optional schema helpers (list/info) if you want the agent to peek at tables/columns\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_community.tools.sql_database.tool import ListSQLDatabaseTool, InfoSQLDatabaseTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a73ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_connector import langchain_groq_llm_connector\n",
    "\n",
    "llm = langchain_groq_llm_connector(\"Insert your API Key here\",\"openai/gpt-oss-120b\")\n",
    "llm = llm.bind(\n",
    "    tools=[{\"type\":\"browser_search\"},{\"type\":\"code_interpreter\"}],\n",
    "    tool_choice=\"auto\",\n",
    "    reasoning_effort=\"medium\",\n",
    "    top_p=1,\n",
    "    max_completion_tokens=8192,\n",
    ")\n",
    "\n",
    "\n",
    "# ---- Safety knobs for SQL and results ----------------------------------------\n",
    "SQL_TIMEOUT_SECONDS = 300      # wall clock per query\n",
    "SQL_MAX_RETURN_ROWS = 100000   # cap result rows to avoid huge pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c46091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_loader import prepare_citibike_database\n",
    "\n",
    "DB_PATH, conn, run_query = prepare_citibike_database()\n",
    "SQLITE_URI = f\"sqlite:///{DB_PATH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfdfc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"outputs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c1e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 2) HELPERS ----------------------------------------------------------------\n",
    "def _is_read_only(sql: str) -> bool:\n",
    "    \"\"\"Allow only SELECT and PRAGMA table_info (read-only).\"\"\"\n",
    "    forbidden = r\"\\b(INSERT|UPDATE|DELETE|DROP|ALTER|CREATE|REPLACE|TRUNCATE|ATTACH|DETACH|VACUUM|BEGIN|COMMIT)\\b\"\n",
    "    if re.search(forbidden, sql, flags=re.IGNORECASE):\n",
    "        return False\n",
    "    if re.search(r\"\\bPRAGMA\\b\", sql, re.IGNORECASE) and \"table_info\" not in sql.lower():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def _add_limit_if_missing(sql: str, limit: int) -> str:\n",
    "    \"\"\"Inject a LIMIT if the user didn't specify one on SELECT queries.\"\"\"\n",
    "    q = sql.strip().rstrip(\";\")\n",
    "    if not q.lower().startswith(\"select\"):\n",
    "        return q + \";\"\n",
    "    if re.search(r\"\\blimit\\b\", q, flags=re.IGNORECASE):\n",
    "        return q + \";\"\n",
    "    return f\"SELECT * FROM (\\n{q}\\n) LIMIT {limit};\"\n",
    "\n",
    "# -- 2a) SQL tool: run a safe read-only query and save to CSV/JSON --------------\n",
    "@tool(\"run_sqlite_query\", return_direct=False)\n",
    "def run_sqlite_query(query: str, limit: int = SQL_MAX_RETURN_ROWS) -> str:\n",
    "    \"\"\"\n",
    "    Run a **read-only** SQL query against the local SQLite DB.\n",
    "    Safety:\n",
    "      - Only SELECT or PRAGMA table_info allowed.\n",
    "      - LIMIT injected if missing (for SELECT).\n",
    "    Returns a small JSON string holding paths & preview.\n",
    "    \"\"\"\n",
    "    if not _is_read_only(query):\n",
    "        return json.dumps({\"error\": \"Only read-only queries allowed (SELECT/PRAGMA table_info).\"})\n",
    "\n",
    "    # Ensure limit for safety\n",
    "    if query.strip().lower().startswith(\"select\") and \"limit\" not in query.lower():\n",
    "        query = _add_limit_if_missing(query, limit)\n",
    "\n",
    "    # Execute with sqlite3; pandas makes it easy to load the result\n",
    "    conn = sqlite3.connect(DB_PATH, check_same_thread=False)\n",
    "    conn.execute(f\"PRAGMA busy_timeout = {SQL_TIMEOUT_SECONDS*1000};\")\n",
    "    try:\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"SQLite error: {e}\"})\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "    # Persist to disk for the next tool\n",
    "    csv_path  = os.path.join(\"outputs\", \"sql_result.csv\")\n",
    "    json_path = os.path.join(\"outputs\", \"sql_result.json\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    df.to_json(json_path, orient=\"records\")\n",
    "\n",
    "    preview_md = df.head(10).to_markdown(index=False) if not df.empty else \"(no rows)\"\n",
    "    return json.dumps({\n",
    "        \"rows\": len(df),\n",
    "        \"columns\": list(df.columns),\n",
    "        \"csv_path\": csv_path,\n",
    "        \"json_path\": json_path,\n",
    "        \"preview\": preview_md,\n",
    "        \"sql_used\": query\n",
    "    })\n",
    "\n",
    "# -- 2b) Pandas tool: simple groupby/aggregate pipeline -------------------------\n",
    "@tool(\"pandas_aggregate\", return_direct=False)\n",
    "def pandas_aggregate(csv_path: str,\n",
    "                     analysis_type: str = \"groupby_aggregate\",\n",
    "                     groupby: Optional[List[str]] = None,\n",
    "                     metrics: Optional[List[Dict[str, str]]] = None,\n",
    "                     filters: Optional[List[Dict[str, Any]]] = None) -> str:\n",
    "    \"\"\"\n",
    "    Load CSV produced by the SQL tool and compute a final table.\n",
    "    Supports:\n",
    "      - analysis_type=\"groupby_aggregate\" (default) with:\n",
    "        * groupby: list of columns\n",
    "        * metrics: list of {\"column\":..., \"agg\": one of sum|mean|count|max|min|median|std|nunique}\n",
    "        * filters: optional list of {\"column\":..,\"op\":..,\"value\":..}\n",
    "      - analysis_type=\"describe\" as a fallback summary\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        return json.dumps({\"error\": f\"CSV not found: {csv_path}\"})\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Quick filter helper\n",
    "    def _apply_filters(df, filters):\n",
    "        if not filters: return df\n",
    "        out = df.copy()\n",
    "        for f in filters:\n",
    "            col, op, val = f.get(\"column\"), f.get(\"op\"), f.get(\"value\")\n",
    "            if col not in out.columns:  # skip unknown columns\n",
    "                continue\n",
    "            if op in (\"==\",\"=\",\"eq\"):\n",
    "                out = out[out[col] == val]\n",
    "            elif op in (\">\",\">=\",\"<\",\"<=\"):\n",
    "                try:\n",
    "                    out = out.query(f\"{col} {op} @val\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "            elif op == \"in\":\n",
    "                val = val if isinstance(val, list) else [val]\n",
    "                out = out[out[col].isin(val)]\n",
    "            elif op == \"contains\":\n",
    "                out = out[out[col].astype(str).str.contains(str(val), na=False)]\n",
    "        return out\n",
    "\n",
    "    if analysis_type == \"describe\":\n",
    "        final_df = df.describe(include=\"all\").reset_index()\n",
    "    else:\n",
    "        # Default: groupby_aggregate\n",
    "        df2 = _apply_filters(df, filters)\n",
    "        if not groupby or not metrics:\n",
    "            final_df = df2.head(50)  # if underspecified, just show sample\n",
    "        else:\n",
    "            agg_map = {}\n",
    "            for m in metrics:\n",
    "                col = m.get(\"column\")\n",
    "                agg = m.get(\"agg\",\"sum\").lower()\n",
    "                if col in df2.columns:\n",
    "                    agg_map.setdefault(col, []).append(agg)\n",
    "            if not agg_map:\n",
    "                final_df = df2.head(50)\n",
    "            else:\n",
    "                out = df2.groupby(groupby).agg(agg_map)\n",
    "                out.columns = [\"__\".join(x if isinstance(x, tuple) else (x,)) if isinstance(x, tuple) else x\n",
    "                               for x in out.columns.values]\n",
    "                final_df = out.reset_index()\n",
    "\n",
    "    result_csv  = os.path.join(\"outputs\", \"final_result.csv\")\n",
    "    result_json = os.path.join(\"outputs\", \"final_result.json\")\n",
    "    final_df.to_csv(result_csv, index=False)\n",
    "    final_df.to_json(result_json, orient=\"records\")\n",
    "\n",
    "    return json.dumps({\n",
    "        \"final_csv\": result_csv,\n",
    "        \"final_json\": result_json,\n",
    "        \"rows\": len(final_df),\n",
    "        \"columns\": list(final_df.columns),\n",
    "        \"preview\": final_df.head(15).to_markdown(index=False)\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39acd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NEW: Visualization tools (PNG charts via matplotlib/seaborn, Map heatmaps via folium)\n",
    "# =============================================================================\n",
    "import os, json, math, datetime as _dt\n",
    "from typing import Optional, List, Dict, Any, Union\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Headless plotting\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seaborn for common statistical plots (installed in most envs)\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except Exception:\n",
    "    sns = None  # We'll degrade gracefully if seaborn is missing\n",
    "\n",
    "# Folium for HTML map heatmaps\n",
    "try:\n",
    "    import folium\n",
    "    from folium.plugins import HeatMap\n",
    "except Exception as e:\n",
    "    folium = None\n",
    "    HeatMap = None\n",
    "\n",
    "OUTPUTS_DIR = \"outputs\"\n",
    "os.makedirs(OUTPUTS_DIR, exist_ok=True)\n",
    "\n",
    "def _now_ts():\n",
    "    return _dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def _load_dataframe(path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Data not found: {path}\")\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in (\".csv\",):\n",
    "        return pd.read_csv(path)\n",
    "    if ext in (\".json\",):\n",
    "        return pd.read_json(path, orient=\"records\")\n",
    "    if ext in (\".parquet\",):\n",
    "        return pd.read_parquet(path)\n",
    "    # Fallback: try CSV\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def _apply_filters(df: pd.DataFrame, filters: Optional[List[Dict[str, Any]]]) -> pd.DataFrame:\n",
    "    if not filters:\n",
    "        return df\n",
    "    out = df.copy()\n",
    "    for f in filters:\n",
    "        col, op, val = f.get(\"column\"), f.get(\"op\"), f.get(\"value\")\n",
    "        if col not in out.columns:\n",
    "            continue\n",
    "        if op in (\"==\",\"=\",\"eq\"):\n",
    "            out = out[out[col] == val]\n",
    "        elif op in (\">\",\">=\",\"<\",\"<=\",\"!=\",\"<>\"):\n",
    "            try:\n",
    "                out = out.query(f\"`{col}` {op.replace('<>','!=')} @val\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        elif op == \"in\":\n",
    "            val = val if isinstance(val, list) else [val]\n",
    "            out = out[out[col].isin(val)]\n",
    "        elif op == \"contains\":\n",
    "            out = out[out[col].astype(str).str.contains(str(val), na=False)]\n",
    "        elif op == \"between\":\n",
    "            try:\n",
    "                lo, hi = val\n",
    "                out = out[(out[col] >= lo) & (out[col] <= hi)]\n",
    "            except Exception:\n",
    "                pass\n",
    "    return out\n",
    "\n",
    "def _normalize_list_arg(v: Optional[Union[str, List[str]]]) -> Optional[List[str]]:\n",
    "    if v is None:\n",
    "        return None\n",
    "    if isinstance(v, list):\n",
    "        return v\n",
    "    return [str(v)]\n",
    "\n",
    "# -- 2c) Visualization tool: static charts to PNG ------------------------------\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool(\"plot_visualization\", return_direct=False)\n",
    "def plot_visualization(data_path: str,\n",
    "                       chart_type: str,\n",
    "                       x: Optional[str] = None,\n",
    "                       y: Optional[Union[str, List[str]]] = None,\n",
    "                       hue: Optional[str] = None,\n",
    "                       agg: Optional[str] = None,\n",
    "                       bins: Optional[int] = None,\n",
    "                       kde: bool = False,\n",
    "                       figsize: Optional[List[float]] = None,\n",
    "                       title: Optional[str] = None,\n",
    "                       xlabel: Optional[str] = None,\n",
    "                       ylabel: Optional[str] = None,\n",
    "                       rotate_xticks: Optional[int] = None,\n",
    "                       filters: Optional[List[Dict[str, Any]]] = None,\n",
    "                       output_filename: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Create a static chart from a CSV/JSON table and save it as PNG in the 'outputs' directory.\n",
    "\n",
    "    INPUT SCHEMA (named args – pass them directly, NOT as a single JSON string):\n",
    "    - data_path (str): Path to CSV/JSON produced by prior tools (e.g., outputs/final_result.csv).\n",
    "    - chart_type (str): One of:\n",
    "        \"line\", \"area\", \"bar\", \"barh\", \"scatter\", \"hist\", \"box\", \"violin\", \"pie\".\n",
    "    - x (str, optional): Column name for x-axis or categories.\n",
    "    - y (str | List[str], optional): Column(s) for y-axis (multi-series allowed for line/bar/area).\n",
    "    - hue (str, optional): Categorical column to split/colour series (mainly for seaborn charts).\n",
    "    - agg (str, optional): If grouping needed for bar/pie (e.g., \"sum\",\"mean\",\"count\",\"median\",\"max\",\"min\").\n",
    "    - bins (int, optional): For histogram bins (chart_type=\"hist\").\n",
    "    - kde (bool, optional): Overlay KDE on histogram (requires seaborn).\n",
    "    - figsize (List[float], optional): [width, height] in inches. Default [10,6].\n",
    "    - title, xlabel, ylabel (str, optional): Labels and title.\n",
    "    - rotate_xticks (int, optional): Rotate x tick labels by N degrees (e.g., 45).\n",
    "    - filters (List[dict], optional): Row filters, same format as pandas_aggregate:\n",
    "        [{\"column\":\"col\",\"op\":\"==|>|>=|<|<=|!=|in|contains|between\",\"value\":...}, ...]\n",
    "    - output_filename (str, optional): PNG filename to use. Default auto: outputs/plot_<ts>.png\n",
    "\n",
    "    OUTPUT (JSON str):\n",
    "    {\n",
    "      \"image_path\": \"outputs/plot_20250101_120000.png\",\n",
    "      \"chart_type\": \"bar\",\n",
    "      \"columns_used\": [\"col_x\",\"col_y1\",\"col_y2\"],\n",
    "      \"rows_plotted\": 123,\n",
    "      \"notes\": \"Short status message\"\n",
    "    }\n",
    "\n",
    "    BEHAVIOR NOTES FOR THE AGENT:\n",
    "    - If the user's request mentions charts/plots/visuals that are NOT geospatial heatmaps,\n",
    "      prefer this tool. Use folium heatmaps via 'map_heatmap' for lat/lng intensity maps.\n",
    "    - If multiple y series are requested, pass y as a list.\n",
    "    - For pie charts: set x=categorical, y=numeric, and (optionally) agg=\"sum\" to aggregate.\n",
    "    - Always point data_path to a file created earlier (e.g., from run_sqlite_query or pandas_aggregate).\n",
    "    - If seaborn is missing, the tool will fall back to matplotlib where possible.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = _load_dataframe(data_path)\n",
    "        df = _apply_filters(df, filters)\n",
    "        if df.empty:\n",
    "            return json.dumps({\"error\": \"No data to plot after applying filters.\"})\n",
    "\n",
    "        chart_type = (chart_type or \"\").lower().strip()\n",
    "        if chart_type not in {\"line\",\"area\",\"bar\",\"barh\",\"scatter\",\"hist\",\"box\",\"violin\",\"pie\"}:\n",
    "            return json.dumps({\"error\": f\"Unsupported chart_type: {chart_type}\"})\n",
    "\n",
    "        # Figure setup\n",
    "        w, h = (figsize if (isinstance(figsize, list) and len(figsize) == 2) else [10, 6])\n",
    "        fig = plt.figure(figsize=(float(w), float(h)))\n",
    "        ax = plt.gca()\n",
    "\n",
    "        used_cols = []\n",
    "        y_cols = _normalize_list_arg(y)\n",
    "\n",
    "        # Helper to aggregate if requested\n",
    "        def maybe_aggregate(_df, _x, _ys):\n",
    "            if not agg:\n",
    "                return _df\n",
    "            if _x and _x in _df.columns:\n",
    "                agg_fun = agg.lower()\n",
    "                valid = {\"sum\",\"mean\",\"count\",\"median\",\"max\",\"min\",\"std\",\"nunique\"}\n",
    "                if agg_fun not in valid:\n",
    "                    return _df\n",
    "                if not _ys:\n",
    "                    # aggregate y-less case (e.g., count by x)\n",
    "                    return _df.groupby(_x).size().reset_index(name=\"count\")\n",
    "                # aggregate over provided y columns\n",
    "                use = [c for c in _ys if c in _df.columns]\n",
    "                if not use:\n",
    "                    return _df\n",
    "                grouped = _df.groupby(_x)[use].agg(agg_fun)\n",
    "                if isinstance(grouped, pd.Series):\n",
    "                    grouped = grouped.to_frame(name=f\"{use[0]}__{agg_fun}\")\n",
    "                else:\n",
    "                    grouped.columns = [f\"{c}__{agg_fun}\" for c in grouped.columns]\n",
    "                return grouped.reset_index()\n",
    "            return _df\n",
    "\n",
    "        # Build the chart\n",
    "        if chart_type in {\"line\",\"area\",\"bar\",\"barh\",\"scatter\"}:\n",
    "            if x is None:\n",
    "                return json.dumps({\"error\": f\"'x' is required for {chart_type} chart.\"})\n",
    "            if y_cols is None and chart_type != \"scatter\":\n",
    "                # allow single-series line/bar/area when y missing? Use all numeric, up to 3 series\n",
    "                numeric = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c]) and c != x]\n",
    "                y_cols = numeric[:3] if numeric else None\n",
    "            df2 = maybe_aggregate(df, x, y_cols)\n",
    "            if y_cols and all(c in df2.columns for c in y_cols):\n",
    "                used_cols = [x] + y_cols\n",
    "                plot_df = df2[[x] + y_cols].copy()\n",
    "                if chart_type in {\"line\",\"area\",\"bar\",\"barh\"}:\n",
    "                    # melt for seaborn friendly multi-series\n",
    "                    m = plot_df.melt(id_vars=[x], var_name=\"series\", value_name=\"value\")\n",
    "                    if sns:\n",
    "                        if chart_type == \"line\":\n",
    "                            g = sns.lineplot(data=m, x=x, y=\"value\", hue=\"series\", ax=ax)\n",
    "                        elif chart_type == \"area\":\n",
    "                            # stack area by series\n",
    "                            # We'll pivot and use matplotlib stacked area\n",
    "                            pvt = m.pivot(index=x, columns=\"series\", values=\"value\").fillna(0)\n",
    "                            pvt.plot.area(ax=ax)\n",
    "                        elif chart_type in {\"bar\",\"barh\"}:\n",
    "                            if chart_type == \"bar\":\n",
    "                                g = sns.barplot(data=m, x=x, y=\"value\", hue=\"series\", ax=ax, errorbar=None)\n",
    "                            else:\n",
    "                                # horizontal bars\n",
    "                                # For horizontal, swap axes via matplotlib\n",
    "                                # Aggregate again to wide then plot horizontally\n",
    "                                pvt = m.pivot(index=x, columns=\"series\", values=\"value\").fillna(0)\n",
    "                                pvt.plot(kind=\"barh\", ax=ax)\n",
    "                        else:\n",
    "                            pass\n",
    "                    else:\n",
    "                        # Pure matplotlib fallback\n",
    "                        if chart_type == \"line\":\n",
    "                            for s, grp in m.groupby(\"series\"):\n",
    "                                ax.plot(grp[x], grp[\"value\"], label=s)\n",
    "                            ax.legend()\n",
    "                        elif chart_type == \"area\":\n",
    "                            pvt = m.pivot(index=x, columns=\"series\", values=\"value\").fillna(0)\n",
    "                            ax.stackplot(pvt.index, pvt.T.values, labels=pvt.columns)\n",
    "                            ax.legend(loc=\"upper left\", bbox_to_anchor=(1,1))\n",
    "                        elif chart_type in {\"bar\",\"barh\"}:\n",
    "                            pvt = m.pivot(index=x, columns=\"series\", values=\"value\").fillna(0)\n",
    "                            if chart_type == \"bar\":\n",
    "                                pvt.plot(kind=\"bar\", ax=ax)\n",
    "                            else:\n",
    "                                pvt.plot(kind=\"barh\", ax=ax)\n",
    "                elif chart_type == \"scatter\":\n",
    "                    y1 = y_cols[0] if y_cols else None\n",
    "                    if not y1:\n",
    "                        return json.dumps({\"error\": \"For scatter, please provide y (numeric) column.\"})\n",
    "                    used_cols = [x, y1] + ([hue] if hue else [])\n",
    "                    if sns:\n",
    "                        sns.scatterplot(data=df2, x=x, y=y1, hue=hue, ax=ax)\n",
    "                    else:\n",
    "                        if hue and hue in df2.columns:\n",
    "                            for k, grp in df2.groupby(hue):\n",
    "                                ax.scatter(grp[x], grp[y1], label=str(k), alpha=0.9)\n",
    "                            ax.legend()\n",
    "                        else:\n",
    "                            ax.scatter(df2[x], df2[y1])\n",
    "            else:\n",
    "                return json.dumps({\"error\": f\"Could not resolve y columns: {y_cols}\"})\n",
    "\n",
    "        elif chart_type in {\"hist\"}:\n",
    "            # Histogram over one numeric column (y) or choose first numeric\n",
    "            col = (y_cols or [None])[0]\n",
    "            if col is None:\n",
    "                numeric = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "                if not numeric:\n",
    "                    return json.dumps({\"error\": \"No numeric columns available for histogram.\"})\n",
    "                col = numeric[0]\n",
    "            used_cols = [col]\n",
    "            if sns:\n",
    "                sns.histplot(df[col], bins=bins or 30, kde=kde, ax=ax)\n",
    "            else:\n",
    "                ax.hist(df[col].dropna().values, bins=bins or 30)\n",
    "\n",
    "        elif chart_type in {\"box\",\"violin\"}:\n",
    "            if not x or not y_cols:\n",
    "                return json.dumps({\"error\": f\"'x' and 'y' are required for {chart_type}.\"})\n",
    "            col = y_cols[0]\n",
    "            used_cols = [x, col]\n",
    "            if sns:\n",
    "                if chart_type == \"box\":\n",
    "                    sns.boxplot(data=df, x=x, y=col, hue=hue, ax=ax)\n",
    "                else:\n",
    "                    sns.violinplot(data=df, x=x, y=col, hue=hue, ax=ax, inner=\"quartile\")\n",
    "            else:\n",
    "                # Simple matplotlib boxplot fallback (no hue)\n",
    "                df.boxplot(column=col, by=x, ax=ax)\n",
    "                ax.get_figure().suptitle(\"\")  # remove automatic suptitle\n",
    "\n",
    "        elif chart_type == \"pie\":\n",
    "            # Expect x=categorical (labels), y=numeric (values). Can aggregate with agg.\n",
    "            if not x or not y_cols:\n",
    "                return json.dumps({\"error\": \"Pie chart requires 'x' (labels) and 'y' (numeric).\"})\n",
    "            col = y_cols[0]\n",
    "            df2 = df[[x, col]].copy()\n",
    "            if agg:\n",
    "                fun = getattr(df2.groupby(x)[col], agg.lower(), None)\n",
    "                if callable(fun):\n",
    "                    df2 = fun().reset_index()\n",
    "            df2 = df2.sort_values(col, ascending=False)\n",
    "            used_cols = [x, col]\n",
    "            ax.pie(df2[col].values, labels=df2[x].astype(str).values, autopct=\"%1.1f%%\", startangle=90)\n",
    "            ax.axis(\"equal\")\n",
    "\n",
    "        # Labels & cosmetics\n",
    "        if title: ax.set_title(title)\n",
    "        if xlabel and hasattr(ax, \"set_xlabel\"): ax.set_xlabel(xlabel)\n",
    "        if ylabel and hasattr(ax, \"set_ylabel\"): ax.set_ylabel(ylabel)\n",
    "        if rotate_xticks:\n",
    "            for tick in ax.get_xticklabels():\n",
    "                tick.set_rotation(rotate_xticks)\n",
    "\n",
    "        # Save PNG\n",
    "        png_path = output_filename or os.path.join(OUTPUTS_DIR, f\"plot_{_now_ts()}.png\")\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(png_path, dpi=160)\n",
    "        plt.close(fig)\n",
    "\n",
    "        rows_plotted = len(df) if chart_type != \"hist\" else df[used_cols[0]].notna().sum()\n",
    "        return json.dumps({\n",
    "            \"image_path\": png_path,\n",
    "            \"chart_type\": chart_type,\n",
    "            \"columns_used\": used_cols,\n",
    "            \"rows_plotted\": int(rows_plotted),\n",
    "            \"notes\": \"Chart created successfully.\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            plt.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return json.dumps({\"error\": f\"plot_visualization failed: {e}\"})\n",
    "\n",
    "\n",
    "# -- 2d) Geospatial heatmap tool: Folium HTML heatmap --------------------------\n",
    "@tool(\"map_heatmap\", return_direct=False)\n",
    "def map_heatmap(data_path: str,\n",
    "                lat_col: str = \"lat\",\n",
    "                lng_col: str = \"lng\",\n",
    "                weight_col: Optional[str] = None,\n",
    "                radius: int = 12,\n",
    "                blur: int = 15,\n",
    "                min_opacity: float = 0.4,\n",
    "                max_zoom: int = 18,\n",
    "                tiles: str = \"OpenStreetMap\",\n",
    "                gradient: Optional[Dict[float, str]] = None,\n",
    "                filters: Optional[List[Dict[str, Any]]] = None,\n",
    "                output_filename: Optional[str] = None,\n",
    "                zoom_start: Optional[int] = None) -> str:\n",
    "    \"\"\"\n",
    "    Create an interactive heatmap (HTML) from latitude/longitude data using Folium.\n",
    "\n",
    "    INPUT SCHEMA (named args – pass them directly):\n",
    "    - data_path (str): CSV/JSON table with at least latitude & longitude columns.\n",
    "    - lat_col (str): Name of latitude column (default: \"lat\"; accepts \"latitude\").\n",
    "    - lng_col (str): Name of longitude column (default: \"lng\"; accepts \"lon\",\"long\",\"longitude\").\n",
    "    - weight_col (str, optional): Column with weights/intensity (numeric).\n",
    "    - radius (int): Point radius for heat kernel (default 12).\n",
    "    - blur (int): Amount of blur (default 15).\n",
    "    - min_opacity (float): Min opacity of the heatmap layer (default 0.4).\n",
    "    - max_zoom (int): Max zoom level for heat layer (default 18).\n",
    "    - tiles (str): Base map tiles, e.g., \"OpenStreetMap\", \"CartoDB positron\", \"Stamen Toner\".\n",
    "    - gradient (dict, optional): {0.2:\"#00f\", 0.4:\"#0ff\", 0.6:\"#0f0\", 0.8:\"#ff0\", 1.0:\"#f00\"}.\n",
    "    - filters (List[dict], optional): Same filter schema as other tools.\n",
    "    - output_filename (str, optional): HTML filename; default auto outputs/heatmap_<ts>.html\n",
    "    - zoom_start (int, optional): Initial zoom if not auto-fitting to bounds.\n",
    "\n",
    "    OUTPUT (JSON str):\n",
    "    {\n",
    "      \"map_html\": \"outputs/heatmap_20250101_120500.html\",\n",
    "      \"points_plotted\": 1420,\n",
    "      \"bounds\": [[min_lat,min_lng],[max_lat,max_lng]],\n",
    "      \"notes\": \"Open this HTML file in a browser to view the map.\"\n",
    "    }\n",
    "\n",
    "    BEHAVIOR NOTES FOR THE AGENT:\n",
    "    - Use this tool ONLY when the user asks for intensity/heatmap over coordinates or geospatial hotspots.\n",
    "    - The output is an HTML file (NOT a PNG). Return the HTML path to the user.\n",
    "    - Ensure the input table has valid lat/lng columns (common aliases are handled).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if folium is None or HeatMap is None:\n",
    "            return json.dumps({\"error\": \"folium not available in this environment.\"})\n",
    "\n",
    "        df = _load_dataframe(data_path)\n",
    "        df = _apply_filters(df, filters)\n",
    "\n",
    "        # resolve column aliases\n",
    "        lat_candidates = [lat_col, \"latitude\", \"LAT\", \"Latitude\"]\n",
    "        lng_candidates = [lng_col, \"lon\", \"long\", \"longitude\", \"LON\", \"Longitude\"]\n",
    "        lat_name = next((c for c in lat_candidates if c in df.columns), None)\n",
    "        lng_name = next((c for c in lng_candidates if c in df.columns), None)\n",
    "        if not lat_name or not lng_name:\n",
    "            return json.dumps({\"error\": f\"Lat/Lng columns not found. Tried {lat_candidates} / {lng_candidates}.\"})\n",
    "\n",
    "        # clean and keep only valid points\n",
    "        coords = df[[lat_name, lng_name] + ([weight_col] if weight_col and weight_col in df.columns else [])].copy()\n",
    "        coords = coords.dropna(subset=[lat_name, lng_name])\n",
    "        # remove clearly invalid coordinates\n",
    "        coords = coords[(coords[lat_name].between(-90, 90)) & (coords[lng_name].between(-180, 180))]\n",
    "        if coords.empty:\n",
    "            return json.dumps({\"error\": \"No valid coordinates found to plot.\"})\n",
    "\n",
    "        # center / bounds\n",
    "        min_lat, max_lat = coords[lat_name].min(), coords[lat_name].max()\n",
    "        min_lng, max_lng = coords[lng_name].min(), coords[lng_name].max()\n",
    "        center = [(min_lat + max_lat) / 2.0, (min_lng + max_lng) / 2.0]\n",
    "\n",
    "        m = folium.Map(location=center, tiles=tiles, zoom_start=zoom_start or 5, control_scale=True)\n",
    "\n",
    "        # Prepare data for HeatMap\n",
    "        if weight_col and weight_col in coords.columns:\n",
    "            heat_data = coords[[lat_name, lng_name, weight_col]].values.tolist()\n",
    "        else:\n",
    "            heat_data = coords[[lat_name, lng_name]].values.tolist()\n",
    "\n",
    "        HeatMap(\n",
    "            heat_data,\n",
    "            radius=radius,\n",
    "            blur=blur,\n",
    "            min_opacity=min_opacity,\n",
    "            max_zoom=max_zoom,\n",
    "            gradient=gradient\n",
    "        ).add_to(m)\n",
    "\n",
    "        # fit to bounds if not provided\n",
    "        try:\n",
    "            m.fit_bounds([[min_lat, min_lng], [max_lat, max_lng]])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        html_path = output_filename or os.path.join(OUTPUTS_DIR, f\"heatmap_{_now_ts()}.html\")\n",
    "        m.save(html_path)\n",
    "\n",
    "        return json.dumps({\n",
    "            \"map_html\": html_path,\n",
    "            \"points_plotted\": int(len(heat_data)),\n",
    "            \"bounds\": [[float(min_lat), float(min_lng)], [float(max_lat), float(max_lng)]],\n",
    "            \"notes\": \"Open the HTML file in a browser to view the interactive heatmap.\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"map_heatmap failed: {e}\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1705e89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 3) PROMPT + AGENT ---------------------------------------------------------\n",
    "# Optional: give the agent quick access to schema overview and table info.\n",
    "db = SQLDatabase.from_uri(SQLITE_URI, sample_rows_in_table_info=2)\n",
    "list_tables_tool = ListSQLDatabaseTool(db=db)\n",
    "info_table_tool  = InfoSQLDatabaseTool(db=db)\n",
    "\n",
    "SYSTEM_RULES = textwrap.dedent(\"\"\"\n",
    "You are a careful data analyst working with a local SQLite database.\n",
    "You MUST follow these rules:\n",
    "\n",
    "- SAFETY: Only run read-only SQL (SELECT or PRAGMA table_info). No writes or DDL.\n",
    "- EFFICIENCY: Avoid huge pulls. If the user didn't specify limits/time windows, start small.\n",
    "- CHOOSE METHOD: Decide the simplest correct analysis (groupby aggregate, describe, or basic stats).\n",
    "- VISUALIZATION: \n",
    "  - Use `plot_visualization` for standard charts (line, bar, scatter, histogram, box, pie).\n",
    "  - Use `map_heatmap` for geospatial heatmaps over lat/lng coordinates.\n",
    "  - Only create visuals if they add value to the user's request.\n",
    "- PIPELINE:\n",
    "  1) In case any metrics are requested to be calculated, clarify your formula for that metrics first.\n",
    "  2) For the metric's only, try to search online for refernces of those metrics and clarify your references.\n",
    "  3) (Optional) Inspect schema with list_tables/info if unsure.\n",
    "  4) Write a SELECT query with the minimal set of columns and filters needed.\n",
    "  5) Call `run_sqlite_query` to execute and save results.\n",
    "  6) If aggregation is needed, call `pandas_aggregate` with a structured spec.\n",
    "  7) Create output according to exact guidelines in the OUTPUT FORMAT section below.\n",
    "  8) If visualization is helpful, call `plot_visualization` for any visualization or `map_heatmap` for creating heatmaps.\n",
    "- OUTPUT FORMAT:\n",
    "  - PLAN SECTION: Start with a short PLAN (bullets). It should outline the assumptions you have made, as well as definition of any metrics you've created.\n",
    "  - SQL CODE SECTION: Show the SQL used (fenced code). It should be clear what was the SQL queries you've used to reach to the results.\n",
    "  - RESULTS SECTION: Show the of your final result and output in a table section.\n",
    "  - SUMMARY SECTION: End with a crisp, non-verbose paragraph describing the data to the user.\n",
    "\"\"\").strip()\n",
    "\n",
    "# The prompt ensures the agent builds a simple plan + uses tools in-order\n",
    "PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_RULES),\n",
    "    (\"human\", \"User analysis request: {analysis_request}\\n\\nProceed.\"),\n",
    "    MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "# Attach both tools to the same agent\n",
    "tools = [list_tables_tool, info_table_tool, run_sqlite_query, pandas_aggregate, plot_visualization, map_heatmap]\n",
    "\n",
    "# Create a tool-calling agent with our prompt\n",
    "agent_runnable = create_tool_calling_agent(llm, tools, PROMPT)\n",
    "\n",
    "# Wrap it in an executor so we can capture intermediate steps (the “action trace”)\n",
    "executor = AgentExecutor(\n",
    "    agent=agent_runnable,\n",
    "    tools=tools,\n",
    "    verbose=True,                     # prints a readable trace in the cell output\n",
    "    return_intermediate_steps=True,   # lets us programmatically inspect tool calls\n",
    "    max_iterations=16,\n",
    "    handle_parsing_errors=True,\n",
    ")\n",
    "print(\"Agent ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a0e952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 4) RUN THE AGENT ----------------------------------------------------------\n",
    "# Try your own request here. A few examples:\n",
    "# \"What is the average utilization rate of the bikes for each station in Palo Alto? Meaning that how much each station's bikes are utilized by time?\"\n",
    "# \"What are the top 10 stations by departures on weekdays 7–10 AM?\"\n",
    "# \"Which routes between stations have the highest subscriber share compared to customers?\"\n",
    "# \"What share of rides are done by commuters (7-10 AM weekdays) vs non-commuters?\"\n",
    "# \"Create a heatmap of areas with the highest share of commuters (7-10 AM weekdays) starting their rides there. Meaning the areas that have the highest share of rides starting there in the morning commute time.\"\n",
    "# \"Generate a line graph comparing the utilization rate (number of daily trips per bike capacity of each station) of stations overtime between the cities during september of 2013.\"\n",
    "\n",
    "analysis_request = \"Create a heatmap of areas with the highest share of commuters (7-10 AM weekdays) starting their rides there. Meaning the areas that have the highest share of rides starting there in the morning commute time.\"\n",
    "\n",
    "result = executor.invoke({\"analysis_request\": analysis_request})\n",
    "\n",
    "# The AgentExecutor returns:\n",
    "# - 'output' (final text answer)\n",
    "# - 'intermediate_steps' ([(AgentAction, observation), ...]) = our action trace\n",
    "print(\"\\n=== FINAL ANSWER ===\")\n",
    "print(result[\"output\"])\n",
    "\n",
    "print(\"\\n=== ACTION TRACE (tools & observations) ===\")\n",
    "for i, step in enumerate(result[\"intermediate_steps\"], 1):\n",
    "    action, observation = step\n",
    "    print(f\"\\n-- Step {i}:\")\n",
    "    print(\"Tool:\", getattr(action, \"tool\", \"<unknown>\"))\n",
    "    print(\"Tool Input:\", getattr(action, \"tool_input\", {}))\n",
    "    # Observations can be large JSON strings; truncate for readability\n",
    "    obs_str = str(observation)\n",
    "    print(\"Observation:\", (obs_str[:800] + \"...\") if len(obs_str) > 800 else obs_str)\n",
    "    print(\"Final Answer:\", result[\"output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defc52dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(result[\"output\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72731f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
