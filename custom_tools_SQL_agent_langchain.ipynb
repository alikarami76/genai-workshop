{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a9d0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/alikarami76/genai-workshop/refs/heads/main/db_loader.py\n",
    "!wget -q https://raw.githubusercontent.com/alikarami76/genai-workshop/refs/heads/main/llm_connector.py\n",
    "!wget -q https://raw.githubusercontent.com/alikarami76/genai-workshop/refs/heads/main/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941a2c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe2c6ca",
   "metadata": {},
   "source": [
    "# Custom SQL Agent Workshop\n",
    "\n",
    "We are extending the Citi Bike commuter-pass case study by showing how to scaffold a LangChain SQL agent with guardrailed custom tools. The goal is to help workshop learners see how LLM-driven analytics can surface insights that inspire a targeted commuter offering for current \"Customer\" riders.\n",
    "\n",
    "We'll walk through the full stack—from imports to agent execution—so you can narrate why each component exists and how it supports Retrieval-Augmented Generation (RAG) over the Citi Bike database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a25f954",
   "metadata": {},
   "source": [
    "## Load core libraries\n",
    "\n",
    "This cell pulls in everything the agent relies on: standard Python utilities, pandas for lightweight data wrangling, and the LangChain components that wire prompts, tools, and the SQL agent together. Call out how the optional schema helpers (`ListSQLDatabaseTool`, `InfoSQLDatabaseTool`) let the agent inspect tables before composing queries—an important safety habit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cbfc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libs\n",
    "import os, re, glob, json, sqlite3, textwrap\n",
    "from datetime import datetime\n",
    "from IPython.display import Markdown, display\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Data\n",
    "import pandas as pd\n",
    "\n",
    "# LangChain core + tools\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain.tools import tool\n",
    "\n",
    "# Optional schema helpers (list/info) if you want the agent to peek at tables/columns\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_community.tools.sql_database.tool import ListSQLDatabaseTool, InfoSQLDatabaseTool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9508e20e",
   "metadata": {},
   "source": [
    "## Configure the Groq LLM and sampling controls\n",
    "\n",
    "We connect to the Groq-backed `qwen3-32b` model using our helper, then bind runtime settings that govern how the model reasons:\n",
    "- `tools=[{\"type\": \"browser_search\"}, {\"type\": \"code_interpreter\"}]` exposes built-in Groq capabilities alongside the SQL tools we'll register later.\n",
    "- `tool_choice=\"auto\"` lets the agent decide when to reach for each tool based on the conversation.\n",
    "- `reasoning_effort=\"medium\"` balances depth and latency—great for live demos.\n",
    "- `top_p=1` keeps nucleus sampling wide open. In practice you can dial this down (or pair with `temperature`) to reduce creative variance during production runs.\n",
    "- `max_completion_tokens=8192` caps how long responses may be. Adjust this if you're expecting especially long summaries.\n",
    "\n",
    "**About other knobs:**\n",
    "- `top_k` (not set here) limits how many candidate tokens the sampler considers; lowering it forces the model to stay closer to the most probable continuations.\n",
    "- `temperature` defaults to the connector's setting. Lower values (≈0-0.3) make outputs deterministic—ideal when you want reproducible SQL plans.\n",
    "\n",
    "Encourage learners to treat these parameters as their primary levers for controlling creativity, determinism, and cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a73ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_connector import langchain_groq_llm_connector\n",
    "\n",
    "llm = langchain_groq_llm_connector(\"Insert your API Key here\",\"openai/gpt-oss-20b\")\n",
    "llm = llm.bind(\n",
    "    tools=[{\"type\":\"browser_search\"},{\"type\":\"code_interpreter\"}],\n",
    "    tool_choice=\"auto\",\n",
    "    reasoning_effort=\"medium\",\n",
    "    top_p=1,\n",
    "    max_completion_tokens=8192,\n",
    ")\n",
    "\n",
    "\n",
    "# ---- Safety knobs for SQL and results ----------------------------------------\n",
    "SQL_TIMEOUT_SECONDS = 300      # wall clock per query\n",
    "SQL_MAX_RETURN_ROWS = 100000   # cap result rows to avoid huge pulls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345c400d",
   "metadata": {},
   "source": [
    "## Point to the Citi Bike database\n",
    "\n",
    "We reuse the pre-staged SQLite file so every participant queries the same snapshot. Framing this through the RAG lens: the database is our **retrieval store**. The agent will decide which SQL it needs, fetch structured rows, and then the LLM will **generate** the narrative explanation grounded in those results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c46091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_loader import prepare_citibike_database\n",
    "\n",
    "DB_PATH, conn, run_query = prepare_citibike_database()\n",
    "SQLITE_URI = f\"sqlite:///{DB_PATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c83136c",
   "metadata": {},
   "source": [
    "## Prepare an output workspace\n",
    "\n",
    "Saving result artifacts (CSVs, JSON, Markdown) into `outputs/` makes the toolchain composable. Downstream tools—and learners exploring past runs—can pick up the cached files without rerunning expensive queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfdfc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"outputs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed2f8f8",
   "metadata": {},
   "source": [
    "## Understand the helper functions and tools\n",
    "\n",
    "This section defines the custom tooling our agent will call. Walk through the structure with learners:\n",
    "- `_is_read_only` and `_add_limit_if_missing` are safety rails that filter out destructive SQL and enforce row limits.\n",
    "- `run_sqlite_query` is a LangChain tool that executes vetted SQL, writes the results to disk, and returns a JSON payload with previews. Emphasize how it preserves provenance by echoing the actual SQL used.\n",
    "- `pandas_aggregate` chains on the saved CSV to perform higher-level group-bys, filters, or descriptive stats, then publishes tidy outputs for later steps.\n",
    "\n",
    "Highlight that each tool returns machine-readable JSON—this is crucial for predictable tool chaining and is a best practice students should emulate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c1e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 2) HELPERS ----------------------------------------------------------------\n",
    "def _is_read_only(sql: str) -> bool:\n",
    "    \"\"\"Allow only SELECT and PRAGMA table_info (read-only).\"\"\"\n",
    "    forbidden = r\"\\b(INSERT|UPDATE|DELETE|DROP|ALTER|CREATE|REPLACE|TRUNCATE|ATTACH|DETACH|VACUUM|BEGIN|COMMIT)\\b\"\n",
    "    if re.search(forbidden, sql, flags=re.IGNORECASE):\n",
    "        return False\n",
    "    if re.search(r\"\\bPRAGMA\\b\", sql, re.IGNORECASE) and \"table_info\" not in sql.lower():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def _add_limit_if_missing(sql: str, limit: int) -> str:\n",
    "    \"\"\"Inject a LIMIT if the user didn't specify one on SELECT queries.\"\"\"\n",
    "    q = sql.strip().rstrip(\";\")\n",
    "    if not q.lower().startswith(\"select\"):\n",
    "        return q + \";\"\n",
    "    if re.search(r\"\\blimit\\b\", q, flags=re.IGNORECASE):\n",
    "        return q + \";\"\n",
    "    return f\"SELECT * FROM (\\n{q}\\n) LIMIT {limit};\"\n",
    "\n",
    "# -- 2a) SQL tool: run a safe read-only query and save to CSV/JSON --------------\n",
    "@tool(\"run_sqlite_query\", return_direct=False)\n",
    "def run_sqlite_query(query: str, limit: int = SQL_MAX_RETURN_ROWS) -> str:\n",
    "    \"\"\"\n",
    "    Run a **read-only** SQL query against the local SQLite DB.\n",
    "    Safety:\n",
    "      - Only SELECT or PRAGMA table_info allowed.\n",
    "      - LIMIT injected if missing (for SELECT).\n",
    "    Returns a small JSON string holding paths & preview.\n",
    "    \"\"\"\n",
    "    if not _is_read_only(query):\n",
    "        return json.dumps({\"error\": \"Only read-only queries allowed (SELECT/PRAGMA table_info).\"})\n",
    "\n",
    "    # Ensure limit for safety\n",
    "    if query.strip().lower().startswith(\"select\") and \"limit\" not in query.lower():\n",
    "        query = _add_limit_if_missing(query, limit)\n",
    "\n",
    "    # Execute with sqlite3; pandas makes it easy to load the result\n",
    "    conn = sqlite3.connect(DB_PATH, check_same_thread=False)\n",
    "    conn.execute(f\"PRAGMA busy_timeout = {SQL_TIMEOUT_SECONDS*1000};\")\n",
    "    try:\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"SQLite error: {e}\"})\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "    # Persist to disk for the next tool\n",
    "    csv_path  = os.path.join(\"outputs\", \"sql_result.csv\")\n",
    "    json_path = os.path.join(\"outputs\", \"sql_result.json\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    df.to_json(json_path, orient=\"records\")\n",
    "\n",
    "    preview_md = df.head(10).to_markdown(index=False) if not df.empty else \"(no rows)\"\n",
    "    return json.dumps({\n",
    "        \"rows\": len(df),\n",
    "        \"columns\": list(df.columns),\n",
    "        \"csv_path\": csv_path,\n",
    "        \"json_path\": json_path,\n",
    "        \"preview\": preview_md,\n",
    "        \"sql_used\": query\n",
    "    })\n",
    "\n",
    "# -- 2b) Pandas tool: simple groupby/aggregate pipeline -------------------------\n",
    "@tool(\"pandas_aggregate\", return_direct=False)\n",
    "def pandas_aggregate(csv_path: str,\n",
    "                     analysis_type: str = \"groupby_aggregate\",\n",
    "                     groupby: Optional[List[str]] = None,\n",
    "                     metrics: Optional[List[Dict[str, str]]] = None,\n",
    "                     filters: Optional[List[Dict[str, Any]]] = None) -> str:\n",
    "    \"\"\"\n",
    "    Load CSV produced by the SQL tool and compute a final table.\n",
    "    Supports:\n",
    "      - analysis_type=\"groupby_aggregate\" (default) with:\n",
    "        * groupby: list of columns\n",
    "        * metrics: list of {\"column\":..., \"agg\": one of sum|mean|count|max|min|median|std|nunique}\n",
    "        * filters: optional list of {\"column\":..,\"op\":..,\"value\":..}\n",
    "      - analysis_type=\"describe\" as a fallback summary\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        return json.dumps({\"error\": f\"CSV not found: {csv_path}\"})\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Quick filter helper\n",
    "    def _apply_filters(df, filters):\n",
    "        if not filters: return df\n",
    "        out = df.copy()\n",
    "        for f in filters:\n",
    "            col, op, val = f.get(\"column\"), f.get(\"op\"), f.get(\"value\")\n",
    "            if col not in out.columns:  # skip unknown columns\n",
    "                continue\n",
    "            if op in (\"==\",\"=\",\"eq\"):\n",
    "                out = out[out[col] == val]\n",
    "            elif op in (\">\",\">=\",\"<\",\"<=\"):\n",
    "                try:\n",
    "                    out = out.query(f\"{col} {op} @val\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "            elif op == \"in\":\n",
    "                val = val if isinstance(val, list) else [val]\n",
    "                out = out[out[col].isin(val)]\n",
    "            elif op == \"contains\":\n",
    "                out = out[out[col].astype(str).str.contains(str(val), na=False)]\n",
    "        return out\n",
    "\n",
    "    if analysis_type == \"describe\":\n",
    "        final_df = df.describe(include=\"all\").reset_index()\n",
    "    else:\n",
    "        # Default: groupby_aggregate\n",
    "        df2 = _apply_filters(df, filters)\n",
    "        if not groupby or not metrics:\n",
    "            final_df = df2.head(50)  # if underspecified, just show sample\n",
    "        else:\n",
    "            agg_map = {}\n",
    "            for m in metrics:\n",
    "                col = m.get(\"column\")\n",
    "                agg = m.get(\"agg\",\"sum\").lower()\n",
    "                if col in df2.columns:\n",
    "                    agg_map.setdefault(col, []).append(agg)\n",
    "            if not agg_map:\n",
    "                final_df = df2.head(50)\n",
    "            else:\n",
    "                out = df2.groupby(groupby).agg(agg_map)\n",
    "                out.columns = [\"__\".join(x if isinstance(x, tuple) else (x,)) if isinstance(x, tuple) else x\n",
    "                               for x in out.columns.values]\n",
    "                final_df = out.reset_index()\n",
    "\n",
    "    result_csv  = os.path.join(\"outputs\", \"final_result.csv\")\n",
    "    result_json = os.path.join(\"outputs\", \"final_result.json\")\n",
    "    final_df.to_csv(result_csv, index=False)\n",
    "    final_df.to_json(result_json, orient=\"records\")\n",
    "\n",
    "    return json.dumps({\n",
    "        \"final_csv\": result_csv,\n",
    "        \"final_json\": result_json,\n",
    "        \"rows\": len(final_df),\n",
    "        \"columns\": list(final_df.columns),\n",
    "        \"preview\": final_df.head(15).to_markdown(index=False)\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbfe934",
   "metadata": {},
   "source": [
    "## Assemble the prompt, tools, and agent\n",
    "\n",
    "Here we wire everything together:\n",
    "- `SYSTEM_RULES` is the **system prompt**—effectively the agent's prefix message. It encodes durable behavior (safety, workflow order, required output sections). Because it persists across turns, remind learners that system prompts trump user prompts when conflicts arise.\n",
    "- The human template (`\"User analysis request: {analysis_request}\"`) is the **user prompt** for this run. It conveys the immediate question while inheriting all guardrails from the system message.\n",
    "- The `OUTPUT FORMAT` portion inside `SYSTEM_RULES` acts like explicit **format instructions**, guiding the agent to emit PLAN / SQL / RESULTS / SUMMARY blocks. Keeping formatting directions separate from business logic is a best practice.\n",
    "- `MessagesPlaceholder(\"agent_scratchpad\")` is where LangChain tucks intermediate Thought/Action/Observation steps so the agent can reason iteratively.\n",
    "- `tools = [...]` collects both LangChain-native schema helpers and our custom SQL+pandas utilities. This is the toolbox the agent may activate.\n",
    "- `AgentExecutor(..., max_iterations=8)` limits how many tool invocations the agent can make before giving up—a safeguard against runaway loops. Encourage tuning this alongside `temperature`/`top_p` when debugging stubborn prompts.\n",
    "\n",
    "When discussing best practices for writing system prompts, stress clarity (bullet lists beat prose), sequencing (spell out the workflow), and explicit failure modes (what to do when data is missing, how to keep outputs concise).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1705e89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 3) PROMPT + AGENT ---------------------------------------------------------\n",
    "# Optional: give the agent quick access to schema overview and table info.\n",
    "db = SQLDatabase.from_uri(SQLITE_URI, sample_rows_in_table_info=2)\n",
    "list_tables_tool = ListSQLDatabaseTool(db=db)\n",
    "info_table_tool  = InfoSQLDatabaseTool(db=db)\n",
    "\n",
    "SYSTEM_RULES = textwrap.dedent(\"\"\"\n",
    "You are a careful data analyst working with a local SQLite database.\n",
    "You MUST follow these rules:\n",
    "\n",
    "- SAFETY: Only run read-only SQL (SELECT or PRAGMA table_info). No writes or DDL.\n",
    "- EFFICIENCY: Avoid huge pulls. If the user didn't specify limits/time windows, start small.\n",
    "- CHOOSE METHOD: Decide the simplest correct analysis (groupby aggregate, describe, or basic stats).\n",
    "- PIPELINE:\n",
    "  1) In case any metrics are requested to be calculated, clarify your formula for that metrics first.\n",
    "  2) For the metric's only, try to search online for refernces of those metrics and clarify your references.\n",
    "  3) (Optional) Inspect schema with list_tables/info if unsure.\n",
    "  4) Write a SELECT query with the minimal set of columns and filters needed.\n",
    "  5) Call `run_sqlite_query` to execute and save results.\n",
    "  6) If aggregation is needed, call `pandas_aggregate` with a structured spec.\n",
    "  7) Create output according to exact guidelines in the OUTPUT FORMAT section below.\n",
    "- OUTPUT FORMAT:\n",
    "  - PLAN SECTION: Start with a short PLAN (bullets). It should outline the assumptions you have made, as well as definition of any metrics you've created.\n",
    "  - SQL CODE SECTION: Show the SQL used (fenced code). It should be clear what was the SQL queries you've used to reach to the results.\n",
    "  - RESULTS SECTION: Show the of your final result and output in a table section.\n",
    "  - SUMMARY SECTION: End with a crisp, non-verbose paragraph describing the data to the user.\n",
    "\"\"\").strip()\n",
    "\n",
    "# The prompt ensures the agent builds a simple plan + uses tools in-order\n",
    "PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_RULES),\n",
    "    (\"human\", \"User analysis request: {analysis_request}\\n\\nProceed.\"),\n",
    "    MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "# Attach both tools to the same agent\n",
    "tools = [list_tables_tool, info_table_tool, run_sqlite_query, pandas_aggregate]\n",
    "\n",
    "# Create a tool-calling agent with our prompt\n",
    "agent_runnable = create_tool_calling_agent(llm, tools, PROMPT)\n",
    "\n",
    "# Wrap it in an executor so we can capture intermediate steps (the “action trace”)\n",
    "executor = AgentExecutor(\n",
    "    agent=agent_runnable,\n",
    "    tools=tools,\n",
    "    verbose=True,                     # prints a readable trace in the cell output\n",
    "    return_intermediate_steps=True,   # lets us programmatically inspect tool calls\n",
    "    max_iterations=16,\n",
    "    handle_parsing_errors=True,\n",
    ")\n",
    "print(\"Agent ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bc5cb3",
   "metadata": {},
   "source": [
    "## Invoke the agent and inspect activations\n",
    "\n",
    "We set `analysis_request` to a commuter-focused utilization question and call `executor.invoke`. LangChain then:\n",
    "1. Injects the system prompt (our prefix) and user prompt into the LLM.\n",
    "2. Lets the model draft a plan, choose tools, and issue SQL—this is the agent \"activation\" loop you can narrate from the verbose trace.\n",
    "3. Bridges retrieval and generation: `run_sqlite_query` pulls structured data, `pandas_aggregate` can refine it, and the LLM synthesizes the final explanation.\n",
    "\n",
    "Encourage learners to swap in their own commuter-pass hypotheses (e.g., subscriber share on 7–10 AM rides) to see how the tooling adapts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a0e952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 4) RUN THE AGENT ----------------------------------------------------------\n",
    "# Try your own request here. A few examples:\n",
    "# \"What is the average utilization rate of the bikes for each station in Palo Alto? Meaning that how much each station's bikes are utilized by time?\"\n",
    "# \"What are the top 10 stations by departures on weekdays 7–10 AM?\"\n",
    "# \"Which routes between stations have the highest subscriber share compared to customers?\"\n",
    "# \"What share of rides are done by commuters (7-10 AM weekdays) vs non-commuters?\"\n",
    "\n",
    "analysis_request = \"WRITE YOUR REQUEST HERE\"\n",
    "\n",
    "result = executor.invoke({\"analysis_request\": analysis_request})\n",
    "\n",
    "# The AgentExecutor returns:\n",
    "# - 'output' (final text answer)\n",
    "# - 'intermediate_steps' ([(AgentAction, observation), ...]) = our action trace\n",
    "print(\"\\n=== FINAL ANSWER ===\")\n",
    "print(result[\"output\"])\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "markdown_path = os.path.join(\"outputs\", f\"agent_output_{timestamp}.md\")\n",
    "with open(markdown_path, \"w\", encoding=\"utf-8\") as md_file:\n",
    "    md_file.write(result[\"output\"])\n",
    "print(f\"\\nSaved markdown output to {markdown_path}\")\n",
    "\n",
    "print(\"\\n=== ACTION TRACE (tools & observations) ===\")\n",
    "for i, step in enumerate(result[\"intermediate_steps\"], 1):\n",
    "    action, observation = step\n",
    "    print(f\"\\n-- Step {i}:\")\n",
    "    print(\"Tool:\", getattr(action, \"tool\", \"<unknown>\"))\n",
    "    print(\"Tool Input:\", getattr(action, \"tool_input\", {}))\n",
    "    # Observations can be large JSON strings; truncate for readability\n",
    "    obs_str = str(observation)\n",
    "    print(\"Observation:\", (obs_str[:800] + \"...\") if len(obs_str) > 800 else obs_str)\n",
    "    print(\"Final Answer:\", result[\"output\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd571c7c",
   "metadata": {},
   "source": [
    "## Present the polished answer\n",
    "\n",
    "Rendering the final Markdown output keeps the workshop recap clean. It's also a reminder that agents can return structured artifacts—not just plain text—which you can embed in reports or dashboards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defc52dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(result[\"output\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32388c4",
   "metadata": {},
   "source": [
    "## Experiment sandbox\n",
    "\n",
    "Use the empty cell below for live tinkering—try new prompts, inspect saved CSVs, or prototype additional helper tools without touching the core workflow above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72731f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c82b8c06",
   "metadata": {},
   "source": [
    "## Assignment: Add visualization tools and guide the agent\n",
    "\n",
    "For the next exercise, extend this notebook so the agent can produce charts and interactive heatmaps that support the commuter-pass pitch.\n",
    "\n",
    "**Procedure for learners:**\n",
    "1. Share this script with ChatGPT (or your assistant of choice) along with a clear prompt describing the visualization capability you need—graphing plus heatmaps for Citi Bike data.\n",
    "2. State firm guardrails in that prompt so ChatGPT only adds the necessary tooling and leaves existing logic intact. Remind it to follow best practices for LangChain tool design (typed arguments, structured JSON responses, helpful docstrings).\n",
    "3. Provide a couple of example tool definitions in the prompt so the model mimics the structure used here (`run_sqlite_query`, `pandas_aggregate`).\n",
    "4. Review the returned script, plug the new visualization tools into this notebook, and update the system prompt so it instructs the agent when to call them.\n",
    "5. Finally, run the enhanced agent and ask it to generate an interactive heatmap based on the commuter-time routes—then evaluate how well the output supports the pass hypothesis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
