{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cba8e617",
   "metadata": {},
   "source": [
    "# Citi Bike Commuter Pass Workshop\n",
    "\n",
    "Welcome to the hands-on segment of our AI workshop. We will use a LangChain SQL agent to explore Citi Bike trip data while hypothesizing about launching a commuter pass aimed at current \"Customer\" riders. The idea is to offer a discounted, route-specific pass that nudges commuters into becoming \"Subscribers\".\n",
    "\n",
    "Our descriptive analysis path will be:\n",
    "- start by comparing the share of Subscribers vs. Customers across all routes to understand the baseline mix;\n",
    "- narrow the focus to commuter windows (7-10 AM) to see how that mix shifts during peak rides;\n",
    "- inspect where morning trips originate and how the Customer share varies at those times to identify promising origin-destination pairs.\n",
    "\n",
    "Each section below explains the code we run so you can narrate the workflow clearly to participants.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4e37cc",
   "metadata": {},
   "source": [
    "## Align the Python environment\n",
    "\n",
    "We upgrade the LangChain, Groq, and supporting libraries to known versions so everyone in the workshop has a consistent toolkit. Keeping dependencies aligned avoids distracting package mismatch issues when we run the SQL agent live.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b4b1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/alikarami76/genai-workshop/refs/heads/main/db_loader.py\n",
    "!wget -q https://raw.githubusercontent.com/alikarami76/genai-workshop/refs/heads/main/llm_connector.py\n",
    "!wget -q https://raw.githubusercontent.com/alikarami76/genai-workshop/refs/heads/main/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8d8ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd1b729",
   "metadata": {},
   "source": [
    "## Connect to our Groq-hosted LLM\n",
    "\n",
    "We import a helper that authenticates with the Groq API and returns a LangChain-compatible LLM. This model is the reasoning engine that will interpret our prompts, follow the SQL agent instructions, and summarize findings for the group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12c4f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_connector import langchain_groq_llm_connector\n",
    "\n",
    "llm = langchain_groq_llm_connector(\"Input API Key here\",\"meta-llama/llama-4-maverick-17b-128e-instruct\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fde0b1",
   "metadata": {},
   "source": [
    "## Stage the Citi Bike database\n",
    "\n",
    "The helper `prepare_citibike_database` can set up the SQLite file we prepared for the workshop. Here we point directly to the sample database so we can run fast, local queries during the session. This dataset contains the trip records we will mine for subscriber vs. customer behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c5a9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_loader import prepare_citibike_database\n",
    "\n",
    "db_path, conn, run_query = prepare_citibike_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1ece7d",
   "metadata": {},
   "source": [
    "## Import LangChain agent utilities\n",
    "\n",
    "These imports pull in the SQL agent factory, toolkit, and display helpers we'll use later to format outputs inside the notebook. Having them ready keeps the rest of the notebook focused on the analytic story rather than setup boilerplate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0919ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Markdown, HTML, display\n",
    "from langchain.agents import AgentType, create_sql_agent\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.sql_database import SQLDatabase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a434f23",
   "metadata": {},
   "source": [
    "## Author the system prompt (`prefix`)\n",
    "\n",
    "`MSSQL_AGENT_PREFIX` is the agent's system prompt. It sets the standing rules the LLM must follow whenever it reasons about our SQL data: how to construct queries, limit result counts, and return answers. System prompts apply globally across turns, whereas user prompts (like the question we define later) express the specific task we want answered in that moment. By tailoring this prefix, we teach the model to behave like a careful data analyst for the entire workshop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1001ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSSQL_AGENT_PREFIX = \"\"\"\n",
    "\n",
    "You are an agent designed to interact with a SQL database.\n",
    "## Instructions:\n",
    "- Given an input question, create a syntactically correct {dialect} query\n",
    "to run, then look at the results of the query and return the answer.\n",
    "- Unless the user specifies a specific number of examples they wish to\n",
    "obtain, **ALWAYS** limit your query to at most {top_k} results.\n",
    "- You can order the results by a relevant column to return the most\n",
    "interesting examples in the database.\n",
    "- Never query for all the columns from a specific table, only ask for\n",
    "the relevant columns given the question.\n",
    "- You have access to tools for interacting with the database.\n",
    "- You MUST double check your query before executing it.If you get an error\n",
    "while executing a query,rewrite the query and try again.\n",
    "- DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.)\n",
    "to the database.\n",
    "- DO NOT MAKE UP AN ANSWER OR USE PRIOR KNOWLEDGE, ONLY USE THE RESULTS\n",
    "OF THE CALCULATIONS YOU HAVE DONE.\n",
    "- Your response should be in Markdown. However, **when running  a SQL Query\n",
    "in \"Action Input\", do not include the markdown backticks**.\n",
    "Those are only for formatting the response, not for executing the command.\n",
    "- ALWAYS, as part of your final answer, explain how you got to the answer\n",
    "on a section that starts with: \"Explanation:\". Include the SQL query as\n",
    "part of the explanation section.\n",
    "- If the question does not seem related to the database, just return\n",
    "\"I don\\'t know\" as the answer.\n",
    "- Only use the below tools. Only use the information returned by the\n",
    "below tools to construct your query and final answer.\n",
    "- Do not make up table names, only use the tables returned by any of the\n",
    "tools below.\n",
    "\n",
    "## Tools:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3753dc3",
   "metadata": {},
   "source": [
    "## Describe the reasoning format\n",
    "\n",
    "`MSSQL_AGENT_FORMAT_INSTRUCTIONS` tells the agent exactly how to structure its internal Thought/Action/Observation loop using the ReAct pattern. Unlike the system prompt content, these format instructions focus on how the model should explain its reasoning steps, not what domain guidance to follow. Keeping the structure explicit makes the live demo easier to narrate because participants can see each hop the agent takes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9428114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSSQL_AGENT_FORMAT_INSTRUCTIONS = \"\"\"\n",
    "\n",
    "## Use the following format:\n",
    "\n",
    "Question: the input question you must answer.\n",
    "Thought: you should always think about what to do.\n",
    "Action: the action to take, should be one of [{tool_names}].\n",
    "Action Input: the input to the action.\n",
    "Observation: the result of the action.\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer.\n",
    "Final Answer: the final answer to the original input question.\n",
    "\n",
    "Example of Final Answer:\n",
    "<=== Beginning of example\n",
    "\n",
    "Action: query_sql_db\n",
    "Action Input: \n",
    "For each given station, on average how many bikes were available vs docks? And which station had the least bicycles and most docks available on average (or was the busiest station)?\n",
    "\n",
    "Observation:\n",
    "\tavg_available_bikes\tfree_dock_count\tmax_dock_capacity\tname\n",
    "0\t4.73\t6.26\t11\tCastro Street and El Camino Real\n",
    "1\t5.27\t5.72\t11\tCowper at University\n",
    "2\t5.29\t5.69\t11\tSanta Clara at Almaden\n",
    "3\t5.67\t9.32\t15\tCommercial at Montgomery\n",
    "4\t6.09\t8.89\t15\tBroadway St at Battery St\n",
    "5\t6.13\t12.81\t19\t2nd at Folsom\n",
    "6\t6.13\t8.84\t15\tClay at Battery\n",
    "7\t6.22\t4.76\t11\tUniversity and Emerson\n",
    "8\t6.25\t8.73\t15\tEmbarcadero at Vallejo\n",
    "9\t6.36\t8.63\t15\tSan Jose City Hall\n",
    "Thought:I now know the final answer\n",
    "Final Answer: The station with the least bicycles and most docks available on average (or the busiest station) is \"2nd at Folsom\" with an average of 6.13 bikes available and 12.81 free docks.\n",
    "\n",
    "Explanation:\n",
    "I calculated the average number of available bikes and free docks for each station using the provided data. By comparing these averages, I identified \"2nd at Folsom\" as the station with the highest average of free docks, indicating it is likely the busiest station.\n",
    "\n",
    "```sql\n",
    "SELECT ROUND(AVG(status.bikes_available),2) AS avg_available_bikes, \n",
    "            ROUND(AVG(status.docks_available),2) AS free_dock_count,\n",
    "            station.dock_count AS max_dock_capacity,\n",
    "            station.name\n",
    "     FROM status\n",
    "     INNER JOIN station\n",
    "     ON status.station_id = station.id\n",
    "     GROUP BY name\n",
    "     ORDER BY 1, 2 DESC\n",
    "     LIMIT 10;'\"\n",
    "```\n",
    "===> End of Example\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d1940a",
   "metadata": {},
   "source": [
    "## Build the SQL RAG toolkit\n",
    "\n",
    "`SQLDatabase.from_uri` wraps our SQLite file so LangChain can issue SQL queries on demand. The `SQLDatabaseToolkit` then exposes that database as a tool the agent can call. Together they implement Retrieval-Augmented Generation (RAG) over structured data: the agent decides which SQL query will retrieve the relevant facts (retrieval), runs it through the toolkit, and then the LLM generates a natural-language explanation grounded in those query results (generation). This is how we let the model extract knowledge directly from the Citi Bike tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488be807",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = SQLDatabase.from_uri(f'sqlite:///{db_path}')\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e17c24",
   "metadata": {},
   "source": [
    "## Craft the user prompt\n",
    "\n",
    "`QUESTION` is the user prompt we hand to the agent. User prompts are turn-by-turn instructions—in this case, a descriptive request about trip totals and duration by subscription type. We can iteratively swap in commuter-pass-specific questions (e.g., filtering to 7-10 AM windows) during the workshop to walk through the case study narrative while the system prompt keeps the agent disciplined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c462b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample question to test the agent with: \"\"Give me the total number of trips and the average duration based on each subscription type.\"\"\n",
    "QUESTION = \"\"\"\n",
    "\"Give me the share of subscribers vs customers from total trips.\"\n",
    "\"\"\"\n",
    "\n",
    "agent_executor_SQL = create_sql_agent(\n",
    "    prefix=MSSQL_AGENT_PREFIX,\n",
    "    format_instructions = MSSQL_AGENT_FORMAT_INSTRUCTIONS,\n",
    "    llm=llm,\n",
    "    toolkit=toolkit,\n",
    "    top_k=30,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a48392",
   "metadata": {},
   "source": [
    "## Measuring the output manually\n",
    "Here we want to manually use SQL to calculate the results for fact checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ad5ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_results = run_query(\"\"\"WITH tot AS (\n",
    "  SELECT COUNT(*) AS n\n",
    "  FROM trip\n",
    "),\n",
    "dist AS (\n",
    "  SELECT\n",
    "    CASE\n",
    "      WHEN UPPER(subscription_type) LIKE 'SUBSCRIBER%' THEN 'Subscriber'\n",
    "      WHEN UPPER(subscription_type) LIKE 'CUSTOMER%'   THEN 'Customer'\n",
    "      ELSE 'Other/Unknown'\n",
    "    END AS user_type,\n",
    "    COUNT(*) AS trips\n",
    "  FROM trip\n",
    "  GROUP BY 1\n",
    ")\n",
    "SELECT\n",
    "  user_type,\n",
    "  trips,\n",
    "  ROUND(trips * 100.0 / (SELECT n FROM tot), 2) AS pct_of_all_trips\n",
    "FROM dist\n",
    "WHERE user_type IN ('Subscriber','Customer')  -- keep only the two classes\n",
    "ORDER BY trips DESC;\n",
    "\"\"\")\n",
    "manual_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6828fbae",
   "metadata": {},
   "source": [
    "## Run the agent and review the output\n",
    "\n",
    "`agent_executor_SQL.invoke(QUESTION)` kicks off the full loop: the LLM reads the system prompt, follows the format instructions, retrieves data through SQL, and returns an answer. Pause after this cell when teaching to interpret the SQL it ran and link the findings back to the commuter pass hypothesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea138805",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor_SQL.invoke(QUESTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d31c690",
   "metadata": {},
   "source": [
    "## Optional exploration space\n",
    "\n",
    "Use this empty cell to prototype follow-up questions live—such as drilling into commuter routes or testing the impact of different origin-destination pairs—without modifying the main workflow above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49efaca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
